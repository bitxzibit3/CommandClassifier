{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/s/ls4/users/grartem/RL_robots/CommandClassifier\")\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import pyhocon\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "from RobotCommandClassifier.MyMultilabel import (MyMultiLabelClassificationModel,\n",
    "                                                 MyMultiLabelClassificationArgs,\n",
    "                                                 MyBertForMultiLabelSequenceClassification)\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configFileContent = pyhocon.ConfigFactory.parse_file(\"../Configs/CustomML.conf\")\n",
    "CONFIG = configFileContent['MyMultiTiny2_fold1'].as_plain_ordered_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = MyMultiLabelClassificationArgs(**CONFIG[\"Model\"][\"Args\"])\n",
    "num_labels = sum(CONFIG[\"Model\"]['num_sublabels_per_biglabel'])\n",
    "bertconfig = BertConfig.from_pretrained(\"/s/ls4/users/grartem/RL_robots/CommandClassifier/models/MyMultiTiny2/fold_1/models/checkpoint-63780-epoch-10\",\n",
    "                                       num_labels=num_labels, **model_args.config)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/s/ls4/users/grartem/RL_robots/CommandClassifier/models/MyMultiTiny2/fold_1/models/checkpoint-63780-epoch-10\",\n",
    "                                         do_lower_case=model_args.do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MyBertForMultiLabelSequenceClassification.from_pretrained(\"/s/ls4/users/grartem/RL_robots/CommandClassifier/models/MyMultiTiny2/fold_1/models/checkpoint-63780-epoch-10\",\n",
    "                                                                  bertconfig, \n",
    "                                                                  num_sublabels_per_biglabel=CONFIG[\"Model\"][\"num_sublabels_per_biglabel\"],\n",
    "                                                                  add_attention_for_labels=CONFIG[\"Model\"][\"add_attention_for_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyBertForMultiLabelSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(83828, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(2048, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=312, out_features=123, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\"а нука, робот, подь сюды\", \"иди к тому дереву, которое за домом\"]\n",
    "model_inputs = tokenizer.batch_encode_plus(\n",
    "                phrases, return_tensors=\"pt\", padding=True, truncation=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,   312, 50744,   603,    16, 51960,    16,  1464,  1090, 58079,\n",
       "           1758,     3],\n",
       "         [    2, 53967,   322,  3567, 59282,    16,  8284,   650, 37245,     3,\n",
       "              0,     0]], device='cuda:0'),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]], device='cuda:0')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = joblib.load(CONFIG[\"output_dir\"]+\"/onehotenc.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_prases(phrases):\n",
    "    with torch.no_grad():\n",
    "        model_inputs = tokenizer.batch_encode_plus(\n",
    "                    phrases, return_tensors=\"pt\", padding=True, truncation=True\n",
    "                )\n",
    "        model_inputs = {key: value.to(torch.device(\"cuda\")) for key, value in model_inputs.items()}\n",
    "        logits = model(**model_inputs)\n",
    "        logits = logits[0].cpu()\n",
    "        predictions_2 = np.zeros((len(logits), len(enc.categories_)))\n",
    "        for i in range(predictions_2.shape[0]):\n",
    "            shift = 0\n",
    "            for j in range(len(enc.categories_)):\n",
    "                if \"OneHotArgs\" in CONFIG[\"Data\"] and CONFIG[\"Data\"][\"OneHotArgs\"].get('drop', None) == \"first\":\n",
    "                    label_logits = logits[i, shift:shift+len(enc.categories_[j])-1]\n",
    "                    if sum(label_logits>=0.5)==0:\n",
    "                        predictions_2[i,j] = 0\n",
    "                    else:\n",
    "                        predictions_2[i,j] = np.argmax(label_logits)+1\n",
    "                    shift += len(enc.categories_[j])-1\n",
    "                else:\n",
    "                    predictions_2[i,j] = np.argmax(logits[i, shift:shift+len(enc.categories_[j])])\n",
    "                    shift += len(enc.categories_[j])\n",
    "            assert shift==logits.shape[1]\n",
    "    return predictions_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RobotCommandClassifier import utils\n",
    "from RobotCommandClassifier import *\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "train_x_df, train_y_df, valid_x_df, valid_y_df, test_x_df, test_y_df = utils.load_data(**CONFIG[\"Data\"])\n",
    "\n",
    "if CONFIG[\"Data\"].get(\"add_y_to_x\", False):\n",
    "    with open(CONFIG[\"Data\"][\"y_descriptions_path\"], \"r\") as f:\n",
    "        y_descriptioons = json.load(f)\n",
    "    train_x_df = train_x_df[\"y\"].map(lambda y: y_descriptioons[int(y)]) + \": \" + train_x_df[\"x\"]\n",
    "    valid_x_df = valid_x_df[\"y\"].map(lambda y: y_descriptioons[int(y)]) + \": \" + valid_x_df[\"x\"]\n",
    "    test_x_df = test_x_df[\"y\"].map(lambda y: y_descriptioons[int(y)]) + \": \" + test_x_df[\"x\"]\n",
    "\n",
    "if CONFIG[\"Data\"].get(\"predict_label_flag\", False):\n",
    "    if \"y\" in CONFIG[\"Data\"][\"target_columns\"]:\n",
    "        raise ValueError(\"Указан флаг для использования только бинарных лейблов. Предполагается, что в таком случае 'y' не должен ыть среди target_columns.\")\n",
    "    train_y_df[train_y_df!=0] = 1\n",
    "    valid_y_df[valid_y_df!=0] = 1\n",
    "    test_y_df[test_y_df!=0] = 1\n",
    "if CONFIG[\"Type\"] in [\"simple_ml_multilabel_classifier\", \"mymulti_classifier\"]:\n",
    "    if CONFIG[\"Data\"].get(\"predict_label_flag\", False):\n",
    "        labels = train_y_df.values.tolist()\n",
    "    else:\n",
    "        enc = OneHotEncoder(**CONFIG[\"Data\"].get(\"OneHotArgs\", {}))\n",
    "        enc.fit(train_y_df.values)\n",
    "\n",
    "        labels = []\n",
    "        encoded_labels = enc.transform(train_y_df.values).toarray().astype(int)\n",
    "        for i in range(train_y_df.shape[0]):\n",
    "            labels.append(encoded_labels[i].tolist())\n",
    "    train_df = pd.DataFrame(list(zip(train_x_df, labels)))        \n",
    "else:\n",
    "    train_df = pd.concat([train_x_df, train_y_df], axis=1)\n",
    "train_df.columns = [\"text\", \"labels\"]\n",
    "\n",
    "if CONFIG[\"Type\"] == \"simple_ml_classifier\":\n",
    "    num_labels = len(train_y_df.iloc[:,0].unique())\n",
    "else:\n",
    "    num_labels = len(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_for_prases(test_x_df.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [2., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [6., 0., 0., ..., 3., 0., 0.],\n",
       "       [6., 0., 0., ..., 3., 0., 0.],\n",
       "       [6., 0., 0., ..., 3., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2 = []\n",
    "for line in test_x_df.values.tolist():\n",
    "    pred = predict_for_prases([line])\n",
    "    predictions_2.append(pred)\n",
    "predictions_2 = np.array(predictions_2).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.50      0.67         6\n",
      "         1.0       0.60      0.75      0.67         4\n",
      "         2.0       0.93      0.93      0.93        15\n",
      "         3.0       1.00      1.00      1.00         3\n",
      "         6.0       1.00      1.00      1.00       236\n",
      "         8.0       0.00      0.00      0.00         0\n",
      "         9.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98       264\n",
      "   macro avg       0.65      0.60      0.61       264\n",
      "weighted avg       0.99      0.98      0.98       264\n",
      "\n",
      "direction\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       246\n",
      "           1       0.82      0.75      0.78        12\n",
      "           2       0.67      0.67      0.67         3\n",
      "           4       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.98       264\n",
      "   macro avg       0.87      0.85      0.86       264\n",
      "weighted avg       0.98      0.98      0.98       264\n",
      "\n",
      "meters\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       264\n",
      "\n",
      "    accuracy                           1.00       264\n",
      "   macro avg       1.00      1.00      1.00       264\n",
      "weighted avg       1.00      1.00      1.00       264\n",
      "\n",
      "degshours\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       264\n",
      "\n",
      "    accuracy                           1.00       264\n",
      "   macro avg       1.00      1.00      1.00       264\n",
      "weighted avg       1.00      1.00      1.00       264\n",
      "\n",
      "object1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        28\n",
      "           1       1.00      1.00      1.00        29\n",
      "           2       1.00      0.99      0.99        77\n",
      "           3       0.98      0.96      0.97        56\n",
      "           4       0.91      1.00      0.95        10\n",
      "           5       0.95      0.97      0.96        37\n",
      "           6       1.00      0.96      0.98        27\n",
      "\n",
      "    accuracy                           0.98       264\n",
      "   macro avg       0.97      0.98      0.98       264\n",
      "weighted avg       0.98      0.98      0.98       264\n",
      "\n",
      "nearest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       264\n",
      "\n",
      "    accuracy                           1.00       264\n",
      "   macro avg       1.00      1.00      1.00       264\n",
      "weighted avg       1.00      1.00      1.00       264\n",
      "\n",
      "relation1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        39\n",
      "         1.0       1.00      0.99      1.00       121\n",
      "         2.0       1.00      1.00      1.00       104\n",
      "         5.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           1.00       264\n",
      "   macro avg       0.75      0.75      0.75       264\n",
      "weighted avg       1.00      1.00      1.00       264\n",
      "\n",
      "object2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        39\n",
      "           1       1.00      0.97      0.98        33\n",
      "           2       0.93      1.00      0.96        13\n",
      "           3       0.98      0.98      0.98        53\n",
      "           4       0.98      1.00      0.99        59\n",
      "           5       0.98      0.96      0.97        45\n",
      "           6       0.96      1.00      0.98        22\n",
      "\n",
      "    accuracy                           0.98       264\n",
      "   macro avg       0.98      0.98      0.98       264\n",
      "weighted avg       0.98      0.98      0.98       264\n",
      "\n",
      "relation2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        65\n",
      "         1.0       1.00      0.99      0.99       141\n",
      "         2.0       0.98      1.00      0.99        58\n",
      "         4.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       264\n",
      "   macro avg       0.75      0.75      0.75       264\n",
      "weighted avg       1.00      0.99      0.99       264\n",
      "\n",
      "object3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        65\n",
      "           1       0.98      1.00      0.99        49\n",
      "           2       1.00      1.00      1.00        59\n",
      "           3       1.00      0.96      0.98        26\n",
      "           4       1.00      0.96      0.98        26\n",
      "           5       0.97      1.00      0.99        34\n",
      "           6       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.99       264\n",
      "   macro avg       0.99      0.99      0.99       264\n",
      "weighted avg       0.99      0.99      0.99       264\n",
      "\n",
      "self\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       264\n",
      "\n",
      "    accuracy                           1.00       264\n",
      "   macro avg       1.00      1.00      1.00       264\n",
      "weighted avg       1.00      1.00      1.00       264\n",
      "\n",
      "gaze\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      1.00       264\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       264\n",
      "   macro avg       0.50      0.50      0.50       264\n",
      "weighted avg       1.00      0.99      1.00       264\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.0 \t correct_samples_perc\n",
      "98.0 \t [action]_acc\n",
      "61.0 \t [action]_macrof1\n",
      "98.0 \t [direction]_acc\n",
      "86.0 \t [direction]_macrof1\n",
      "100.0 \t [meters]_acc\n",
      "100.0 \t [meters]_macrof1\n",
      "100.0 \t [degshours]_acc\n",
      "100.0 \t [degshours]_macrof1\n",
      "98.0 \t [object1]_acc\n",
      "98.0 \t [object1]_macrof1\n",
      "100.0 \t [nearest]_acc\n",
      "100.0 \t [nearest]_macrof1\n",
      "100.0 \t [relation1]_acc\n",
      "75.0 \t [relation1]_macrof1\n",
      "98.0 \t [object2]_acc\n",
      "98.0 \t [object2]_macrof1\n",
      "99.0 \t [relation2]_acc\n",
      "75.0 \t [relation2]_macrof1\n",
      "99.0 \t [object3]_acc\n",
      "99.0 \t [object3]_macrof1\n",
      "100.0 \t [self]_acc\n",
      "100.0 \t [self]_macrof1\n",
      "99.0 \t [gaze]_acc\n",
      "50.0 \t [gaze]_macrof1\n",
      "87.0 \t avg_macro_f1\n",
      "99.0 \t avg_acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#result = utils.calculate_metrics_2(test_y_df.iloc[:,1:], predictions_2[:,1:], display=True) # исключить Y из оценки\n",
    "result = utils.calculate_metrics_2(test_y_df, predictions, display=True)\n",
    "\n",
    "#result_avg = utils.calculate_metrics(test_y_df.iloc[:,1:], predictions_2[:,1:], config={\n",
    "result_avg = utils.calculate_metrics(test_y_df, predictions, config={\n",
    "    \"report_metrics\": CONFIG[\"Report\"][\"report_metrics\"]\n",
    "})\n",
    "for k, v in result_avg.items():\n",
    "    print(np.round(v*100), \"\\t\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.50      0.67         6\n",
      "         1.0       0.60      0.75      0.67         4\n",
      "         2.0       0.93      0.93      0.93        15\n",
      "         3.0       1.00      1.00      1.00         3\n",
      "         6.0       1.00      1.00      1.00       236\n",
      "         8.0       0.00      0.00      0.00         0\n",
      "         9.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98       264\n",
      "   macro avg       0.65      0.60      0.61       264\n",
      "weighted avg       0.99      0.98      0.98       264\n",
      "\n",
      "direction\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       246\n",
      "           1       0.82      0.75      0.78        12\n",
      "           2       0.67      0.67      0.67         3\n",
      "           4       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.98       264\n",
      "   macro avg       0.87      0.85      0.86       264\n",
      "weighted avg       0.98      0.98      0.98       264\n",
      "\n",
      "meters\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       264\n",
      "\n",
      "    accuracy                           1.00       264\n",
      "   macro avg       1.00      1.00      1.00       264\n",
      "weighted avg       1.00      1.00      1.00       264\n",
      "\n",
      "degshours\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       264\n",
      "\n",
      "    accuracy                           1.00       264\n",
      "   macro avg       1.00      1.00      1.00       264\n",
      "weighted avg       1.00      1.00      1.00       264\n",
      "\n",
      "object1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        28\n",
      "           1       1.00      1.00      1.00        29\n",
      "           2       1.00      0.99      0.99        77\n",
      "           3       0.98      0.96      0.97        56\n",
      "           4       0.91      1.00      0.95        10\n",
      "           5       0.95      0.97      0.96        37\n",
      "           6       1.00      0.96      0.98        27\n",
      "\n",
      "    accuracy                           0.98       264\n",
      "   macro avg       0.97      0.98      0.98       264\n",
      "weighted avg       0.98      0.98      0.98       264\n",
      "\n",
      "nearest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       264\n",
      "\n",
      "    accuracy                           1.00       264\n",
      "   macro avg       1.00      1.00      1.00       264\n",
      "weighted avg       1.00      1.00      1.00       264\n",
      "\n",
      "relation1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        39\n",
      "         1.0       1.00      0.99      1.00       121\n",
      "         2.0       1.00      1.00      1.00       104\n",
      "         5.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           1.00       264\n",
      "   macro avg       0.75      0.75      0.75       264\n",
      "weighted avg       1.00      1.00      1.00       264\n",
      "\n",
      "object2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        39\n",
      "           1       1.00      0.97      0.98        33\n",
      "           2       0.93      1.00      0.96        13\n",
      "           3       0.98      0.98      0.98        53\n",
      "           4       0.98      1.00      0.99        59\n",
      "           5       0.98      0.96      0.97        45\n",
      "           6       0.96      1.00      0.98        22\n",
      "\n",
      "    accuracy                           0.98       264\n",
      "   macro avg       0.98      0.98      0.98       264\n",
      "weighted avg       0.98      0.98      0.98       264\n",
      "\n",
      "relation2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        65\n",
      "         1.0       1.00      0.99      0.99       141\n",
      "         2.0       0.98      1.00      0.99        58\n",
      "         4.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       264\n",
      "   macro avg       0.75      0.75      0.75       264\n",
      "weighted avg       1.00      0.99      0.99       264\n",
      "\n",
      "object3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        65\n",
      "           1       0.98      1.00      0.99        49\n",
      "           2       1.00      1.00      1.00        59\n",
      "           3       1.00      0.96      0.98        26\n",
      "           4       1.00      0.96      0.98        26\n",
      "           5       0.97      1.00      0.99        34\n",
      "           6       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.99       264\n",
      "   macro avg       0.99      0.99      0.99       264\n",
      "weighted avg       0.99      0.99      0.99       264\n",
      "\n",
      "self\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       264\n",
      "\n",
      "    accuracy                           1.00       264\n",
      "   macro avg       1.00      1.00      1.00       264\n",
      "weighted avg       1.00      1.00      1.00       264\n",
      "\n",
      "gaze\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      1.00       264\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       264\n",
      "   macro avg       0.50      0.50      0.50       264\n",
      "weighted avg       1.00      0.99      1.00       264\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.0 \t correct_samples_perc\n",
      "98.0 \t [action]_acc\n",
      "61.0 \t [action]_macrof1\n",
      "98.0 \t [direction]_acc\n",
      "86.0 \t [direction]_macrof1\n",
      "100.0 \t [meters]_acc\n",
      "100.0 \t [meters]_macrof1\n",
      "100.0 \t [degshours]_acc\n",
      "100.0 \t [degshours]_macrof1\n",
      "98.0 \t [object1]_acc\n",
      "98.0 \t [object1]_macrof1\n",
      "100.0 \t [nearest]_acc\n",
      "100.0 \t [nearest]_macrof1\n",
      "100.0 \t [relation1]_acc\n",
      "75.0 \t [relation1]_macrof1\n",
      "98.0 \t [object2]_acc\n",
      "98.0 \t [object2]_macrof1\n",
      "99.0 \t [relation2]_acc\n",
      "75.0 \t [relation2]_macrof1\n",
      "99.0 \t [object3]_acc\n",
      "99.0 \t [object3]_macrof1\n",
      "100.0 \t [self]_acc\n",
      "100.0 \t [self]_macrof1\n",
      "99.0 \t [gaze]_acc\n",
      "50.0 \t [gaze]_macrof1\n",
      "87.0 \t avg_macro_f1\n",
      "99.0 \t avg_acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#result = utils.calculate_metrics_2(test_y_df.iloc[:,1:], predictions_2[:,1:], display=True) # исключить Y из оценки\n",
    "result = utils.calculate_metrics_2(test_y_df, predictions_2, display=True)\n",
    "\n",
    "#result_avg = utils.calculate_metrics(test_y_df.iloc[:,1:], predictions_2[:,1:], config={\n",
    "result_avg = utils.calculate_metrics(test_y_df, predictions_2, config={\n",
    "    \"report_metrics\": CONFIG[\"Report\"][\"report_metrics\"]\n",
    "})\n",
    "for k, v in result_avg.items():\n",
    "    print(np.round(v*100), \"\\t\", k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simptr",
   "language": "python",
   "name": "simptr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
