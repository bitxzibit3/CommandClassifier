{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/s/ls4/users/grartem/RL_robots/CommandClassifier\")\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "import yaml\n",
    "import pyhocon\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import simpletransformers\n",
    "from sklearn.metrics import classification_report\n",
    "from simpletransformers.classification import (\n",
    "    MultiLabelClassificationModel, MultiLabelClassificationArgs,\n",
    "    ClassificationModel, ClassificationArgs\n",
    ")\n",
    "from RobotCommandClassifier.MyMultilabel import MyMultiLabelClassificationModel, MyMultiLabelClassificationArgs\n",
    "from RobotCommandClassifier import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "configFileContent = pyhocon.ConfigFactory.parse_file(\"../Configs/SimpleLM.conf\")\n",
    "CONFIG = configFileContent['rubert_tiny2_labelflag_fold4'].as_plain_ordered_dict()\n",
    "#configFileContent = pyhocon.ConfigFactory.parse_file(\"../Configs/CustomML.conf\")\n",
    "#CONFIG = configFileContent['MyMultiTiny2_fold4'].as_plain_ordered_dict()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# если хотим протестировать на всем тесте, включая фолды, на которых он обучался\n",
    "CONFIG[\"Data\"].pop(\"test_only_on_fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_df, train_y_df, valid_x_df, valid_y_df, test_x_df, test_y_df = utils.load_data(**CONFIG[\"Data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"Type\"] == \"simple_ml_multilabel_classifier\":\n",
    "    model_args = MultiLabelClassificationArgs(**CONFIG[\"Model\"][\"Args\"])\n",
    "    model = MultiLabelClassificationModel(\n",
    "        CONFIG[\"Model\"][\"model_type\"],\n",
    "        CONFIG[\"output_dir\"] + '/models/checkpoint-63780-epoch-10',\n",
    "        num_labels=len(labels[0]),\n",
    "        use_cuda=True,    \n",
    "        args=model_args,\n",
    "    )\n",
    "elif CONFIG[\"Type\"] == \"simple_ml_classifier\":\n",
    "    model_args = ClassificationArgs(**CONFIG[\"Model\"][\"Args\"])\n",
    "    model = ClassificationModel(\n",
    "        CONFIG[\"Model\"][\"model_type\"],\n",
    "        CONFIG[\"output_dir\"] + '/models/checkpoint-63780-epoch-10',\n",
    "        num_labels=len(train_y_df.iloc[:,0].unique()),\n",
    "        use_cuda=True,    \n",
    "        args=model_args,\n",
    "    )\n",
    "elif CONFIG[\"Type\"] == \"mymulti_classifier\":\n",
    "    model_args = MyMultiLabelClassificationArgs(**CONFIG[\"Model\"][\"Args\"])\n",
    "    # Create a MultiLabelClassificationModel\n",
    "    model = MyMultiLabelClassificationModel(\n",
    "        CONFIG[\"Model\"][\"model_type\"],\n",
    "        CONFIG[\"output_dir\"] + '/models/checkpoint-63780-epoch-10',\n",
    "        num_labels=len(labels[0]),\n",
    "        use_cuda=True,\n",
    "        num_sublabels_per_biglabel = CONFIG[\"Model\"][\"num_sublabels_per_biglabel\"],\n",
    "        args=model_args,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"unknown Type of experiment:{}\".format(CONFIG[\"Type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"Data\"].get(\"add_y_to_x\", False):\n",
    "    with open(CONFIG[\"Data\"][\"y_descriptions_path\"], \"r\") as f:\n",
    "        y_descriptioons = json.load(f)\n",
    "    train_x_df['x'] = train_x_df[\"y\"].map(lambda y: y_descriptioons[int(y)]) + \": \" + train_x_df[\"x\"]\n",
    "    valid_x_df[\"x\"] = valid_x_df[\"y\"].map(lambda y: y_descriptioons[int(y)]) + \": \" + valid_x_df[\"x\"]\n",
    "    test_x_df[\"x\"] = test_x_df[\"y\"].map(lambda y: y_descriptioons[int(y)]) + \": \" + test_x_df[\"x\"]\n",
    "if CONFIG[\"Data\"].get(\"predict_label_flag\", False):\n",
    "    if \"y\" in CONFIG[\"Data\"][\"target_columns\"]:\n",
    "        raise ValueError(\"Указан флаг для использования только бинарных лейблов. Предполагается, что в таком случае 'y' не должен ыть среди target_columns.\")\n",
    "    train_y_df[train_y_df!=0] = 1\n",
    "    valid_y_df[valid_y_df!=0] = 1\n",
    "    test_y_df[test_y_df!=0] = 1                \n",
    "if CONFIG[\"Type\"] in [\"simple_ml_multilabel_classifier\", \"mymulti_classifier\"]:\n",
    "    if CONFIG[\"Data\"].get(\"predict_label_flag\", False):\n",
    "        labels = train_y_df.values.tolist()\n",
    "    else:\n",
    "        enc = OneHotEncoder()\n",
    "        enc.fit(train_y_df.values)\n",
    "\n",
    "        labels = []\n",
    "        encoded_labels = enc.transform(train_y_df.values).toarray().astype(int)\n",
    "        for i in range(train_y_df.shape[0]):\n",
    "            labels.append(encoded_labels[i].tolist())\n",
    "    train_df = pd.DataFrame(list(zip(train_x_df, labels)))        \n",
    "else:\n",
    "    train_df = pd.concat([train_x_df, train_y_df], axis=1)\n",
    "train_df.columns = [\"text\", \"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d5c5e8271f40ad8e4a14e5cbccd328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ed093e8cd5452fa71acd80ce93e1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict(test_x_df.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmax of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [297]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m shift \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(enc\u001b[38;5;241m.\u001b[39mcategories_)):\n\u001b[0;32m----> 8\u001b[0m     predictions_2[i,j] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift\u001b[49m\u001b[43m:\u001b[49m\u001b[43mshift\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategories_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     shift \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(enc\u001b[38;5;241m.\u001b[39mcategories_[j])\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/simptr/lib/python3.8/site-packages/numpy/core/fromnumeric.py:1188\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argmax_dispatcher)\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;124;03m    Returns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \n\u001b[1;32m   1187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/simptr/lib/python3.8/site-packages/numpy/core/fromnumeric.py:58\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
     ]
    }
   ],
   "source": [
    "#привести бинарный мультилейбл к мультиклассовому\n",
    "with open(\"../Data/Interim/active_labels_for_y.json\", \"r\") as f:\n",
    "    active_labels_for_y = json.load(f)\n",
    "predictions_2 = np.zeros((len(predictions), len(enc.categories_)))\n",
    "for i in range(len(predictions)):\n",
    "    shift = 0\n",
    "    for j in range(len(enc.categories_)):\n",
    "        predictions_2[i,j] = np.argmax(raw_outputs[i, shift:shift+len(enc.categories_[j])])\n",
    "        shift += len(enc.categories_[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.50      0.62        10\n",
      "           1       0.98      1.00      0.99       253\n",
      "\n",
      "    accuracy                           0.98       263\n",
      "   macro avg       0.91      0.75      0.81       263\n",
      "weighted avg       0.97      0.98      0.97       263\n",
      "\n",
      "direction\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       245\n",
      "           1       1.00      0.83      0.91        18\n",
      "\n",
      "    accuracy                           0.99       263\n",
      "   macro avg       0.99      0.92      0.95       263\n",
      "weighted avg       0.99      0.99      0.99       263\n",
      "\n",
      "meters\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       263\n",
      "\n",
      "    accuracy                           1.00       263\n",
      "   macro avg       1.00      1.00      1.00       263\n",
      "weighted avg       1.00      1.00      1.00       263\n",
      "\n",
      "degshours\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       263\n",
      "\n",
      "    accuracy                           1.00       263\n",
      "   macro avg       1.00      1.00      1.00       263\n",
      "weighted avg       1.00      1.00      1.00       263\n",
      "\n",
      "object1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96        28\n",
      "           1       0.99      1.00      1.00       235\n",
      "\n",
      "    accuracy                           0.99       263\n",
      "   macro avg       1.00      0.96      0.98       263\n",
      "weighted avg       0.99      0.99      0.99       263\n",
      "\n",
      "nearest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       263\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       263\n",
      "   macro avg       0.50      0.50      0.50       263\n",
      "weighted avg       1.00      0.99      1.00       263\n",
      "\n",
      "relation1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        38\n",
      "           1       1.00      1.00      1.00       225\n",
      "\n",
      "    accuracy                           1.00       263\n",
      "   macro avg       1.00      1.00      1.00       263\n",
      "weighted avg       1.00      1.00      1.00       263\n",
      "\n",
      "object2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        38\n",
      "           1       1.00      1.00      1.00       225\n",
      "\n",
      "    accuracy                           1.00       263\n",
      "   macro avg       1.00      1.00      1.00       263\n",
      "weighted avg       1.00      1.00      1.00       263\n",
      "\n",
      "relation2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95        64\n",
      "           1       0.99      0.97      0.98       199\n",
      "\n",
      "    accuracy                           0.98       263\n",
      "   macro avg       0.96      0.98      0.97       263\n",
      "weighted avg       0.98      0.98      0.98       263\n",
      "\n",
      "object3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95        64\n",
      "           1       0.99      0.97      0.98       199\n",
      "\n",
      "    accuracy                           0.98       263\n",
      "   macro avg       0.96      0.98      0.97       263\n",
      "weighted avg       0.98      0.98      0.98       263\n",
      "\n",
      "self\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       263\n",
      "\n",
      "    accuracy                           1.00       263\n",
      "   macro avg       1.00      1.00      1.00       263\n",
      "weighted avg       1.00      1.00      1.00       263\n",
      "\n",
      "gaze\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       263\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       263\n",
      "   macro avg       0.50      0.50      0.50       263\n",
      "weighted avg       1.00      0.99      1.00       263\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.0 \t correct_samples_perc\n",
      "98.0 \t [action]_acc\n",
      "81.0 \t [action]_macrof1\n",
      "99.0 \t [direction]_acc\n",
      "95.0 \t [direction]_macrof1\n",
      "100.0 \t [meters]_acc\n",
      "100.0 \t [meters]_macrof1\n",
      "100.0 \t [degshours]_acc\n",
      "100.0 \t [degshours]_macrof1\n",
      "99.0 \t [object1]_acc\n",
      "98.0 \t [object1]_macrof1\n",
      "99.0 \t [nearest]_acc\n",
      "50.0 \t [nearest]_macrof1\n",
      "100.0 \t [relation1]_acc\n",
      "100.0 \t [relation1]_macrof1\n",
      "100.0 \t [object2]_acc\n",
      "100.0 \t [object2]_macrof1\n",
      "98.0 \t [relation2]_acc\n",
      "97.0 \t [relation2]_macrof1\n",
      "98.0 \t [object3]_acc\n",
      "97.0 \t [object3]_macrof1\n",
      "100.0 \t [self]_acc\n",
      "100.0 \t [self]_macrof1\n",
      "99.0 \t [gaze]_acc\n",
      "50.0 \t [gaze]_macrof1\n",
      "89.0 \t avg_macro_f1\n",
      "99.0 \t avg_acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(CONFIG[\"output_dir\"], \"reports\")):\n",
    "    os.mkdir(os.path.join(CONFIG[\"output_dir\"], \"reports\"))\n",
    "#result = utils.calculate_metrics_2(test_y_df.iloc[:,1:], predictions_2[:,1:], display=True) # исключить Y из оценки\n",
    "result = utils.calculate_metrics_2(test_y_df, predictions_2, display=True)\n",
    "with open(os.path.join(CONFIG[\"output_dir\"], \"reports/epoch-10_classes_report.json\"), \"w\") as f:\n",
    "    json.dump(result, f)\n",
    "\n",
    "#result_avg = utils.calculate_metrics(test_y_df.iloc[:,1:], predictions_2[:,1:], config={\n",
    "result_avg = utils.calculate_metrics(test_y_df, predictions_2, config={\n",
    "    \"report_metrics\": CONFIG[\"Report\"][\"report_metrics\"]\n",
    "})\n",
    "with open(os.path.join(CONFIG[\"output_dir\"], \"reports/epoch-10_avg_report.json\"), \"w\") as f:\n",
    "    json.dump(result_avg, f)\n",
    "for k, v in result_avg.items():\n",
    "    print(np.round(v*100), \"\\t\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# выбрать только возможные или максимально правдоподобные\n",
    "possible_combinations = pd.read_csv(\"../Data/Interim/possible_combinations.csv\")\n",
    "possible_tuples = []\n",
    "for ir, row in possible_combinations.loc[:, CONFIG[\"Data\"][\"target_columns\"]].iterrows():\n",
    "    possible_tuples.append(tuple(row.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No possible tuples for 116\n",
      "[[[8, 6, 0, 0, 0, 8, 0, 2, 4, 1, 6, 0, 0], 11.85870361328125], [[8, 6, 0, 0, 0, 8, 0, 2, 4, 1, 4, 0, 0], 11.9217529296875], [[8, 6, 0, 0, 0, 8, 0, 2, 4, 1, 1, 0, 0], 12.33251953125]]\n"
     ]
    }
   ],
   "source": [
    "# Rule\n",
    "with open(\"../Data/Interim/active_labels_for_y.json\", \"r\") as f:\n",
    "    active_labels_for_y = json.load(f)\n",
    "predictions_3 = []\n",
    "for i in range(len(predictions)):\n",
    "    shift = 0\n",
    "    predicted_tuples = []\n",
    "    for cat_i in range(len(enc.categories_)):        \n",
    "        probs = raw_outputs[i, shift:shift+len(enc.categories_[cat_i])]\n",
    "        chosen_probs = np.where(probs>0.7)[0]\n",
    "        if len(chosen_probs)==0:\n",
    "            #chosen_probs = [np.argmax(probs)]\n",
    "            #print(\"Low confidence\", CONFIG[\"Data\"][\"target_columns\"][cat_i], np.round(probs, 3))\n",
    "            chosen_probs = np.argsort(probs)[-3:]\n",
    "        if cat_i==0:\n",
    "            for cls in chosen_probs:\n",
    "                predicted_tuples.append([[cls], probs[cls]])\n",
    "        else:\n",
    "            if len(chosen_probs)==1:\n",
    "                for tp in predicted_tuples:\n",
    "                    tp[0].append(chosen_probs[0])\n",
    "                    tp[1] += probs[chosen_probs[0]]\n",
    "            elif len(chosen_probs)>1:\n",
    "                #print(i, CONFIG[\"Data\"][\"target_columns\"][cat_i], \"more than 1 prob\", chosen_probs)\n",
    "                new_tuples = []\n",
    "                for tp in predicted_tuples:\n",
    "                    tp_template = deepcopy(tp)\n",
    "                    for cls_i, cls in enumerate(chosen_probs):\n",
    "                        if cls_i==0:\n",
    "                            tp[0].append(cls)\n",
    "                            tp[1] += probs[cls]\n",
    "                        else:\n",
    "                            new_tp = deepcopy(tp_template)\n",
    "                            new_tp[0].append(cls)\n",
    "                            new_tp[1] += probs[cls]\n",
    "                            new_tuples.append(new_tp)\n",
    "                predicted_tuples.extend(new_tuples)\n",
    "        shift += len(enc.categories_[cat_i])\n",
    "    possible_predicted_tuples = []\n",
    "    for tp in predicted_tuples:\n",
    "        assert len(tp[0])==len(CONFIG[\"Data\"][\"target_columns\"])\n",
    "        if tuple(tp[0]) in possible_tuples:\n",
    "            possible_predicted_tuples.append([tp[0], tp[1]])\n",
    "    \n",
    "    if len(possible_predicted_tuples) ==0:\n",
    "        print(\"No possible tuples for\", i)\n",
    "        print(predicted_tuples)\n",
    "        predictions_3.append(sorted(predicted_tuples, key=lambda x: x[1])[-1][0])\n",
    "    else:\n",
    "        predictions_3.append(sorted(possible_predicted_tuples, key=lambda x: x[1])[-1][0])\n",
    "predictions_3 = np.array(predictions_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86        10\n",
      "           1       1.00      0.89      0.94        18\n",
      "           5       0.82      0.82      0.82        11\n",
      "           7       0.92      0.92      0.92        26\n",
      "           8       0.99      0.99      0.99       199\n",
      "          10       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97       264\n",
      "   macro avg       0.76      0.75      0.76       264\n",
      "weighted avg       0.97      0.97      0.97       264\n",
      "\n",
      "action\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.82         7\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.94      0.88      0.91        17\n",
      "           3       0.00      0.00      0.00         1\n",
      "           6       1.00      0.99      0.99       236\n",
      "           9       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97       264\n",
      "   macro avg       0.44      0.48      0.45       264\n",
      "weighted avg       0.97      0.97      0.97       264\n",
      "\n",
      "direction\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       246\n",
      "           1       0.80      0.80      0.80        10\n",
      "           2       1.00      0.57      0.73         7\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.98       264\n",
      "   macro avg       0.76      0.67      0.70       264\n",
      "weighted avg       0.98      0.98      0.98       264\n",
      "\n",
      "meters\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       264\n",
      "\n",
      "    accuracy                           1.00       264\n",
      "   macro avg       1.00      1.00      1.00       264\n",
      "weighted avg       1.00      1.00      1.00       264\n",
      "\n",
      "degshours\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       264\n",
      "\n",
      "    accuracy                           1.00       264\n",
      "   macro avg       1.00      1.00      1.00       264\n",
      "weighted avg       1.00      1.00      1.00       264\n",
      "\n",
      "object1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98        28\n",
      "           1       1.00      0.97      0.99        35\n",
      "           2       1.00      0.85      0.92        27\n",
      "           3       0.89      1.00      0.94        33\n",
      "           4       0.98      1.00      0.99        50\n",
      "           5       0.98      0.98      0.98        59\n",
      "           6       1.00      0.88      0.93        32\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.96       264\n",
      "   macro avg       0.69      0.66      0.67       264\n",
      "weighted avg       0.98      0.96      0.97       264\n",
      "\n",
      "nearest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       264\n",
      "\n",
      "    accuracy                           1.00       264\n",
      "   macro avg       1.00      1.00      1.00       264\n",
      "weighted avg       1.00      1.00      1.00       264\n",
      "\n",
      "relation1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        39\n",
      "           1       0.99      0.99      0.99       147\n",
      "           2       0.97      0.97      0.97        78\n",
      "\n",
      "    accuracy                           0.98       264\n",
      "   macro avg       0.98      0.98      0.98       264\n",
      "weighted avg       0.98      0.98      0.98       264\n",
      "\n",
      "object2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        39\n",
      "           1       0.89      0.94      0.91        33\n",
      "           2       0.97      0.85      0.91        40\n",
      "           3       0.92      0.96      0.94        24\n",
      "           4       0.92      0.98      0.95        49\n",
      "           5       0.97      0.97      0.97        39\n",
      "           6       1.00      0.97      0.99        40\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.95      0.95      0.95       264\n",
      "weighted avg       0.95      0.95      0.95       264\n",
      "\n",
      "relation2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        65\n",
      "           1       0.99      0.99      0.99       150\n",
      "           2       0.98      0.98      0.98        49\n",
      "\n",
      "    accuracy                           0.98       264\n",
      "   macro avg       0.98      0.98      0.98       264\n",
      "weighted avg       0.98      0.98      0.98       264\n",
      "\n",
      "object3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        65\n",
      "           1       0.91      0.93      0.92        44\n",
      "           2       0.75      0.90      0.82        30\n",
      "           3       0.88      0.93      0.90        40\n",
      "           4       0.58      0.54      0.56        13\n",
      "           5       0.96      0.86      0.91        50\n",
      "           6       0.95      0.82      0.88        22\n",
      "\n",
      "    accuracy                           0.90       264\n",
      "   macro avg       0.86      0.85      0.85       264\n",
      "weighted avg       0.90      0.90      0.90       264\n",
      "\n",
      "self\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       264\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           1.00       264\n",
      "   macro avg       0.50      0.50      0.50       264\n",
      "weighted avg       1.00      1.00      1.00       264\n",
      "\n",
      "gaze\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       264\n",
      "\n",
      "    accuracy                           1.00       264\n",
      "   macro avg       1.00      1.00      1.00       264\n",
      "weighted avg       1.00      1.00      1.00       264\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.0 \t correct_samples_perc\n",
      "97.0 \t [y]_acc\n",
      "76.0 \t [y]_macrof1\n",
      "97.0 \t [action]_acc\n",
      "45.0 \t [action]_macrof1\n",
      "98.0 \t [direction]_acc\n",
      "70.0 \t [direction]_macrof1\n",
      "100.0 \t [meters]_acc\n",
      "100.0 \t [meters]_macrof1\n",
      "100.0 \t [degshours]_acc\n",
      "100.0 \t [degshours]_macrof1\n",
      "96.0 \t [object1]_acc\n",
      "67.0 \t [object1]_macrof1\n",
      "100.0 \t [nearest]_acc\n",
      "100.0 \t [nearest]_macrof1\n",
      "98.0 \t [relation1]_acc\n",
      "98.0 \t [relation1]_macrof1\n",
      "95.0 \t [object2]_acc\n",
      "95.0 \t [object2]_macrof1\n",
      "98.0 \t [relation2]_acc\n",
      "98.0 \t [relation2]_macrof1\n",
      "90.0 \t [object3]_acc\n",
      "85.0 \t [object3]_macrof1\n",
      "100.0 \t [self]_acc\n",
      "50.0 \t [self]_macrof1\n",
      "100.0 \t [gaze]_acc\n",
      "100.0 \t [gaze]_macrof1\n",
      "83.0 \t avg_macro_f1\n",
      "98.0 \t avg_acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/s/ls4/users/grartem/anaconda3/envs/simptr/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(CONFIG[\"output_dir\"], \"reports\")):\n",
    "    os.mkdir(os.path.join(CONFIG[\"output_dir\"], \"reports\"))\n",
    "result = utils.calculate_metrics_2(test_y_df, predictions_3, display=True)\n",
    "with open(os.path.join(CONFIG[\"output_dir\"], \"reports/epoch-10_classes_report_rule.json\"), \"w\") as f:\n",
    "    json.dump(result, f)\n",
    "\n",
    "result_avg = utils.calculate_metrics(test_y_df, predictions_3, config={\n",
    "    \"report_metrics\": CONFIG[\"Report\"][\"report_metrics\"]\n",
    "})\n",
    "with open(os.path.join(CONFIG[\"output_dir\"], \"reports/epoch-10_avg_report_rule.json\"), \"w\") as f:\n",
    "    json.dump(result_avg, f)\n",
    "for k, v in result_avg.items():\n",
    "    print(np.round(v*100), \"\\t\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранить ошибки\n",
    "with open(\"../Data/Interim/labels_names.json\", \"r\") as f:\n",
    "    labels_names = json.load(f)\n",
    "predict_df = pd.DataFrame(predictions_2.astype(np.int))\n",
    "predict_df.columns = [c+\"_pred\" for c in test_y_df.columns]\n",
    "predict_df.index = test_y_df.index\n",
    "errors = pd.concat([test_x_df, test_y_df, predict_df], axis=1)\n",
    "for c in errors.columns:\n",
    "    if c in \"x\":\n",
    "        continue\n",
    "    if \"_pred\" in c:\n",
    "        errors[c] = errors[c].map(lambda x: labels_names[c.replace(\"_pred\", \"\")][x])\n",
    "    else:\n",
    "        errors[c] = errors[c].map(lambda x: labels_names[c][x])\n",
    "errors = errors.loc[:, [\"x\"] + [x for c in test_y_df.columns if c!= \"x\" for x in [c,c+\"_pred\"]]]\n",
    "\n",
    "fullDF = pd.read_csv(CONFIG[\"Data\"][\"path_to_df\"])\n",
    "fullDF = fullDF[fullDF[\"subset\"]==\"test\"]\n",
    "errors = pd.concat([errors, fullDF.loc[fullDF.index.isin(errors.index),[\"type\", \"fold\"]]], axis=1)\n",
    "\n",
    "errors.to_csv(\"../Docs/rubert_tiny2_errors_fold4_test.csv\", sep=\";\", encoding=\"cp1251\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simptr",
   "language": "python",
   "name": "simptr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
