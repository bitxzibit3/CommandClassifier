{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/s/ls4/users/grartem/RL_robots/CommandClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from simpletransformers.config.model_args import ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ParsedData = []\n",
    "with open(\"/s/ls4/users/grartem/RL_robots/CommandClassifier/models/CommandParser_tokenCLS_copy-1_ipynb_data5/checkpoint-116700-epoch-20/predict_test_one_cmd_v4_05072022_low_split.jsonlines\", \"r\") as f:\n",
    "    for line in f:\n",
    "        ParsedData.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/s/ls4/users/grartem/RL_robots/CommandClassifier/Data/Interim/labels_names.json\", \"r\") as f:\n",
    "    attribute_names = json.load(f)\n",
    "dictionaries_folder = \"/s/ls4/users/grartem/RL_robots/CommandClassifier/Data/External/dict/\"\n",
    "attributes_dictionaries = {}\n",
    "label_to_dictname = dict(zip([\"A\", \"D\", \"M\", \"DH\", \"ON\", \"N\", \"RN\", \"S\", \"G\"],['action', 'direction', 'meters', 'degshours', 'object', 'nearest', 'relation', 'self', 'gaze']))\n",
    "label_to_attrname = dict(zip([\"A\", \"D\", \"M\", \"DH\", \"ON\", \"N\", \"RN\", \"S\", \"G\"],[\"action\", 'direction', 'meters', 'degshours', 'object1', 'nearest', 'relation1', 'self', 'gaze']))\n",
    "hours_map = {\n",
    "    \"1hour\": \"30\",\n",
    "    \"2hour\": \"60\",\n",
    "    \"3hour\": \"90\",\n",
    "    \"4hour\": \"120\",\n",
    "    \"5hour\": \"150\",\n",
    "    \"6hour\": \"180\",\n",
    "    \"7hour\": \"210\",\n",
    "    \"8hour\": \"240\",\n",
    "    \"9hour\": \"270\",\n",
    "    \"10hour\": \"300\",\n",
    "    \"11hour\": \"330\",\n",
    "    \"12hour\": \"360\",\n",
    "    \"o1hour\": \"15\",\n",
    "    \"o2hour\": \"45\",\n",
    "    \"o3hour\": \"75\",\n",
    "    \"o4hour\": \"105\",\n",
    "    \"o5hour\": \"135\",\n",
    "    \"o6hour\": \"165\",\n",
    "    \"o7hour\": \"195\",\n",
    "    \"o8hour\": \"225\",\n",
    "    \"o9hour\": \"255\",\n",
    "    \"o10hour\": \"285\",\n",
    "    \"o11hour\": \"315\",\n",
    "    \"o12hour\": \"345\",\n",
    "}\n",
    "for phrases_list_file in os.listdir(dictionaries_folder):\n",
    "    with open(dictionaries_folder + phrases_list_file, \"r\") as f:\n",
    "        phrases = f.readlines()\n",
    "    phrases_list_file = phrases_list_file.split(\"_\")\n",
    "    phrases = [x.strip() for x in phrases if x.strip()!=\"\"]\n",
    "    \n",
    "    label = phrases_list_file[0]\n",
    "    if label==\"O\":\n",
    "        continue\n",
    "    attr_name = \"_\".join(phrases_list_file[1:])    \n",
    "    if attr_name in [\"forward\", \"backward\", \"right\", \"left\", \"north\", \"south\", \"east\", \"west\"]:\n",
    "        attr_name = \"dir_\" + attr_name\n",
    "    if \"hour\" in attr_name:\n",
    "        attr_name = hours_map[attr_name]\n",
    "    if attr_name in [\"rotate\", \"move\"]:\n",
    "        attr_name += \"_dir\"\n",
    "    if attr_name in [\"additional_verb\"]:\n",
    "        continue\n",
    "    if attr_name in [\"focus\", \"focus_obj\"]:\n",
    "        attr_name = \"has_gaze_focus_on\"\n",
    "    if attr_name in [\"nearest\"]:\n",
    "        attr_name = \"nearest_from_type\"\n",
    "    if attr_name in [\"after_turn_around\"]: # degshours\n",
    "        attr_name = \"180\"\n",
    "    if attr_name in [\"turn_around\"]: # action\n",
    "        attr_name = \"rotate_on\" # not sure\n",
    "    \n",
    "    if attr_name in [\"degrees\", \"meters\"]:\n",
    "        attr_name = attr_name.replace(\"degrees\", \"degshours\")\n",
    "        if label_to_dictname[label] not in attributes_dictionaries:\n",
    "            attributes_dictionaries[label_to_dictname[label]] = [[],[]]\n",
    "        for val in attribute_names[attr_name]:\n",
    "            if val==\"\":\n",
    "                continue\n",
    "            for postfix in phrases:\n",
    "                phrase = val+\" \" + postfix\n",
    "        \n",
    "                attributes_dictionaries[label_to_dictname[label]][0].append(phrase)\n",
    "                dict_class_id = attribute_names[label_to_attrname[label]].index(val)\n",
    "                attributes_dictionaries[label_to_dictname[label]][1].append(dict_class_id)\n",
    "        continue\n",
    "    if label_to_dictname[label] not in attributes_dictionaries:\n",
    "        attributes_dictionaries[label_to_dictname[label]] = [[],[]]\n",
    "    attributes_dictionaries[label_to_dictname[label]][0].extend(phrases)\n",
    "    dict_class_id = attribute_names[label_to_attrname[label]].index(attr_name)\n",
    "    attributes_dictionaries[label_to_dictname[label]][1].extend([dict_class_id for x in phrases])\n",
    "    \n",
    "    if attr_name in [\"rotate_dir\", \"move_dir\"]:\n",
    "        attr_name = attr_name.replace(\"_dir\", \"_to\")\n",
    "        dict_class_id = attribute_names[label_to_attrname[label]].index(attr_name)\n",
    "        attributes_dictionaries[label_to_dictname[label]][0].extend(phrases)\n",
    "        attributes_dictionaries[label_to_dictname[label]][1].extend([dict_class_id for x in phrases])\n",
    "        attr_name = attr_name.replace(\"_to\", \"_on\")\n",
    "        dict_class_id = attribute_names[label_to_attrname[label]].index(attr_name)\n",
    "        attributes_dictionaries[label_to_dictname[label]][0].extend(phrases)\n",
    "        attributes_dictionaries[label_to_dictname[label]][1].extend([dict_class_id for x in phrases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем словари для каждого атрибута по существующим командам\n",
    "# одинаковые фразы могут относиться к разным классам. тут никуда не денешься\n",
    "from collections import defaultdict\n",
    "attributes_dictionaries = defaultdict(list)\n",
    "attributes_list = [\"action\", 'direction', 'meters', 'degshours', 'object1', 'nearest', 'relation1', 'object2', 'relation2', 'object3', 'self', 'gaze']\n",
    "labels_list = [\"A\", \"D\", \"M\", \"DH\", \"O1\", \"N\", \"R1\", \"O2\", \"R2\", \"O3\", \"S\", \"G\"]\n",
    "dictionaries_names_list = ['action', 'direction', 'meters', 'degshours', 'object', 'nearest', 'relation', 'object', 'relation', 'object', 'self', 'gaze']\n",
    "with open(\"../../Data/Interim/data_v5_split.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = json.loads(line)\n",
    "        for lab, dictionary_name in zip(labels_list,\n",
    "                                      dictionaries_names_list):\n",
    "            phrase = \" \".join([x for x,y in zip(line[0], line[1]) if y==lab])\n",
    "            if phrase!=\"\":\n",
    "                attributes_dictionaries[dictionary_name].append((phrase, line[2][labels_list.index(lab)]))\n",
    "for k in attributes_dictionaries.keys():\n",
    "    attributes_dictionaries[k] = set(attributes_dictionaries[k])\n",
    "for k in attributes_dictionaries.keys():\n",
    "    attributes_dictionaries[k] = [[x[0] for x in attributes_dictionaries[k]], [x[1] for x in attributes_dictionaries[k]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "class SiameseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, attributes_dictionaries, tokenizer, use_cuda = False):\n",
    "        '''\n",
    "        '''\n",
    "        self.attributes_tuples = []\n",
    "        for k in attributes_dictionaries.keys():\n",
    "            for i in range(len(attributes_dictionaries[k][0])):\n",
    "                phrase, cls  = attributes_dictionaries[k][0][i], attributes_dictionaries[k][1][i]\n",
    "                self.attributes_tuples.append((k, phrase, cls))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.use_cuda=use_cuda\n",
    "        #if use_cuda:\n",
    "        #    self.X = self.X.to('cuda') \n",
    "        #    self.y = self.y.to('cuda')\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        #total_len = 0\n",
    "        #for k in self.attributes_dictionaries.keys():\n",
    "        #    total_len += len(self.attributes_dictionaries[k])\n",
    "        return len(self.attributes_tuples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if type(idx)==int:\n",
    "            idx = torch.tensor(idx)\n",
    "        anchor = self.attributes_tuples[idx]\n",
    "        pos_samples = [x for x in self.attributes_tuples if (x[0]==anchor[0]) and (x[2]==anchor[2])]\n",
    "        neg_samples = [x for x in self.attributes_tuples if (x[0]==anchor[0]) and (x[2]!=anchor[2])]\n",
    "        pos_sample = random.sample(pos_samples, 1)[0]\n",
    "        neg_sample = random.sample(neg_samples, 1)[0]\n",
    "        #не проверял 64, да и вообще это костыль\n",
    "        sample = self.tokenizer([anchor[1], pos_sample[1], neg_sample[1]], padding=True, pad_to_multiple_of=8,return_tensors='pt')\n",
    "        if self.use_cuda:\n",
    "            for k in sample.keys():\n",
    "                sample[k] = sample[k].cuda()\n",
    "        sample = {\n",
    "            \"anchor_input_ids\": sample['input_ids'][0],\n",
    "            \"pos_input_ids\": sample['input_ids'][1],\n",
    "            \"neg_input_ids\": sample['input_ids'][2],\n",
    "            \"anchor_token_type_ids\": sample['token_type_ids'][0],\n",
    "            \"pos_token_type_ids\": sample['token_type_ids'][1],\n",
    "            \"neg_token_type_ids\": sample['token_type_ids'][2],\n",
    "            \"anchor_attention_mask\": sample['attention_mask'][0],\n",
    "            \"pos_attention_mask\": sample['attention_mask'][1],\n",
    "            \"neg_attention_mask\": sample['attention_mask'][2]\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# проверка, нет ли моментов, где только 1 пример на класс\n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for sample in dataset.attributes_tuples:\n",
    "    counter[(sample[0], sample[2])]+=1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /s/ls4/groups/g0126/transformers_models/cointegrated/rubert-tiny2/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel, BertTokenizer\n",
    "\n",
    "# Download model and configuration from huggingface.co and cache.\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/s/ls4/groups/g0126/transformers_models/cointegrated/rubert-tiny2/\")\n",
    "model = BertModel.from_pretrained(\"/s/ls4/groups/g0126/transformers_models/cointegrated/rubert-tiny2/\")\n",
    "model.to(\"cuda\")\n",
    "_ = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_dictionaries_for_train = {}\n",
    "for k in attributes_dictionaries.keys():\n",
    "    if k in [\"gaze\", \"nearest\"]:\n",
    "        continue\n",
    "    attributes_dictionaries_for_train[k] = attributes_dictionaries[k]\n",
    "dataset = SiameseDataset(attributes_dictionaries_for_train, tokenizer, use_cuda=True)\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=512,\n",
    "                                          shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1028"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7, device='cuda:0'),\n",
       " tensor(3, device='cuda:0'),\n",
       " tensor(4.3200, device='cuda:0'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzeros_counts = []\n",
    "for sample in dataset:\n",
    "    nonzeros_counts.append(sample[\"anchor_input_ids\"].count_nonzero())\n",
    "    nonzeros_counts.append(sample[\"pos_input_ids\"].count_nonzero())\n",
    "    nonzeros_counts.append(sample[\"neg_input_ids\"].count_nonzero())\n",
    "max(nonzeros_counts), min(nonzeros_counts), sum(nonzeros_counts) / len(nonzeros_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class SiameseNetwork(torch.nn.Module):\n",
    "    def __init__(self, learning_rate, model, **kwargs):\n",
    "        super().__init__()\n",
    "  \n",
    "        self.learning_rate = learning_rate\n",
    "        self._encoder = model\n",
    "        self.loss = torch.nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "  \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "  \n",
    "    def _loss(self, anchor, pos, neg):\n",
    "        return self.loss(anchor, pos, neg)\n",
    "  \n",
    "    def forward(self, anchor_input_ids, pos_input_ids, neg_input_ids,\n",
    "               anchor_token_type_ids, pos_token_type_ids, neg_token_type_ids,\n",
    "               anchor_attention_mask, pos_attention_mask, neg_attention_mask):\n",
    "        anchor = self._encoder(anchor_input_ids, anchor_token_type_ids, anchor_attention_mask)\n",
    "        emb1 = self._encoder(pos_input_ids, pos_token_type_ids, pos_attention_mask)\n",
    "        emb2 = self._encoder(neg_input_ids, neg_token_type_ids, neg_attention_mask)\n",
    "        return anchor[\"pooler_output\"], emb1[\"pooler_output\"], emb2[\"pooler_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seamiseNN = SiameseNetwork(learning_rate=0.00001, model=model)\n",
    "optim = seamiseNN.configure_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_loss': tensor(0.9637, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "{'avg_loss': tensor(0.8792, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "{'avg_loss': tensor(0.8832, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "{'avg_loss': tensor(0.9876, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "{'avg_loss': tensor(0.8277, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for epoch in range(5):\n",
    "    epoch_stat = {}\n",
    "    batch_losses = []\n",
    "    for batch in trainloader:\n",
    "        #with torch.cuda.amp.autocast():    \n",
    "        returns = seamiseNN(**batch)\n",
    "        loss = seamiseNN.loss(*returns)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        batch_losses.append(loss)\n",
    "    epoch_stat[\"avg_loss\"] = sum(batch_losses) / len(batch_losses)\n",
    "    print(epoch_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seamiseNN._encoder.save_pretrained(\"/s/ls4/users/grartem/RL_robots/CommandClassifier/models/Prototype_pretrainedNormalizeModel\",)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simptr",
   "language": "python",
   "name": "simptr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
