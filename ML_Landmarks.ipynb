{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bitxzibit3/CommandClassifier/blob/main/ML_Landmarks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrHMg1kovn6n"
      },
      "source": [
        "## Установка и импорт модулей "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j01RcnPe4vJ",
        "outputId": "44581cde-eb9f-4130-fb83-d7b3f2454d07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.22.1-py2.py3-none-any.whl (203 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.1/203.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=3c7c004752617bf40c493a78cd337fabbe9262621706c834ae97e0db8eea548b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.22.1 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.2\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mQlRyIIUeZCD"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "import albumentations.pytorch as Ap\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms as tf\n",
        "from torchvision.datasets import DatasetFolder \n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "if not os.path.exists('./drive'):\n",
        "    drive.mount('./drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyQdmZ3tSriu"
      },
      "outputs": [],
      "source": [
        "# !pip install pipreqs\n",
        "# !pip install nbconvert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gDv8PQDUO4L"
      },
      "outputs": [],
      "source": [
        "!jupyter nbconvert --output-dir=\"./reqs\" --to script ./drive/MyDrive/Colab\\ Notebooks/ML_Landmarks.ipynb\n",
        "!cd reqs\n",
        "!pipreqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMIsGZjlVkVO"
      },
      "outputs": [],
      "source": [
        "!pip install pipreqsnb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDKH4eIbVowA"
      },
      "outputs": [],
      "source": [
        "!pipreqsnb --savepath ./requirements.txt ./drive/MyDrive/Colab\\ Notebooks/ML_Landmarks.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG61REZqesZs",
        "outputId": "f9d50ad7-6114-4556-a50e-bb0b2889325e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading qudata-gembed-landmarks-210.zip to /content\n",
            " 99% 735M/741M [00:12<00:00, 81.7MB/s]\n",
            "100% 741M/741M [00:12<00:00, 63.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists('/root/.kaggle/kaggle.json'):\n",
        "    !mkdir /root/.kaggle\n",
        "    !cp ./drive/MyDrive/kaggle.json /root/.kaggle/kaggle.json\n",
        "    !kaggle datasets download -d andreybeyn/qudata-gembed-landmarks-210\n",
        "    !unzip -q qudata-gembed-landmarks-210.zip\n",
        "\n",
        "    !mkdir ./models ./models/models ./models/desc\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ak-wNNv1fMs6"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyxHg2A9gaH5"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    project = 'ml_landmarks',\n",
        "    entity = 'ml_landmarks',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbL0U5EtcV-6"
      },
      "source": [
        "# Пайплайн примерно такой:\n",
        "\n",
        "\n",
        "*   Обрабатываем данные:\n",
        "\n",
        "    * Считываем данные, перегоняем в тензоры\n",
        "\n",
        "    * Мб делаем нормализацию (пока нет)\n",
        "\n",
        "    * Разбиваем на тренировочную/валидационную\n",
        "\n",
        "*   Пишем сетки: пробуем менять архитектуру, если совсем голяк - меняем предобработку.\n",
        "\n",
        "*   Попробовать сделать ансамбли: бэггинг!, бустинг.\n",
        "\n",
        "*   Оценивать будем `F1`, скорее всего.\n",
        "\n",
        "*   Если все совсем совсем плохо:\n",
        "\n",
        "    * Пробовать более сильные ансамбли. Если тут голяк - пробовать еще:)\n",
        "    \n",
        "    * Будем пробовать аугментацию, потому что картинок реально мало\n",
        "    \n",
        "    * Можно будет попробовать найти похожую сетку (похожую, исходя из поставленной задачи), и попробовать её зафайнтьюнить.\n",
        "    \n",
        "    * Брать другой датасет. Есть сразу проблемы: они в большинстве своем неразмечены (те, которые я находил).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBJY1ezZXc_k"
      },
      "source": [
        "## Чтение данных\n",
        "\n",
        "Тут я попробовал поиграть с вариантами хранения данных. Где то считывал данные, и хранил уже обработанные тензоры, где то считывал данные, и обрабатывал только при необходимости. Также использовал класс, встроенный в `torchvision`. В конце привел сравнение работы всех классов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOmvaLsLVEAy"
      },
      "source": [
        "В датасете есть черно-белые фотографии, и фотографии, в которых 4 канала, а не 3 по стандарту. Таких картинок немного, поэтому удалим их."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD3XN3wd_Wch",
        "outputId": "1d2622dd-7685-4b8e-d8e9-8749c53ef313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All files: 10515\n",
            "Deleted files: 19\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "init_path = './landmarks/'\n",
        "\n",
        "# Get all filepaths\n",
        "all_files = set()\n",
        "labels = []\n",
        "for path, dirs, files in os.walk(init_path):\n",
        "    if dirs == []:\n",
        "        for file_ in files:\n",
        "            filepath = '/'.join([path, file_])\n",
        "            all_files.add(filepath)\n",
        "    else:\n",
        "        labels.extend(dirs)\n",
        "\n",
        "# Filtering\n",
        "supported_types = ('RGB')\n",
        "\n",
        "incorrect_files = set()\n",
        "for filename in all_files:\n",
        "    img = PIL.Image.open(filename)\n",
        "    if img.mode not in supported_types:\n",
        "        incorrect_files.add(filename)\n",
        "    del img\n",
        "\n",
        "all_files = list(all_files - incorrect_files)\n",
        "print('\\n'.join([f'All files: {len(all_files)}',\n",
        "                 f'Deleted files: {len(incorrect_files)}']))\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnSm8eSra48Z"
      },
      "source": [
        "### FastDataset(читает на лету)\n",
        "Тут я сделал класс датасета с быстрым доступом.\n",
        "\n",
        "Мы сразу по названию файла обрезаем её, делаем из картинки тензор, запоминаем его, потом получаем к нему доступ просто по индексу, не делая никакой предобработки.\n",
        "\n",
        "+: Быстро бегаем\n",
        "\n",
        "-: Долгая инициализация\n",
        "\n",
        "-: В теории, может сожрать всю память"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCHpJcJcGacG"
      },
      "outputs": [],
      "source": [
        "class FastDataset(Dataset):\n",
        "    def __init__(self, mode = 'train', files = all_files, labels = labels,\n",
        "                 transform = None,\n",
        "                 image_shape = (200, 200)):\n",
        "        \n",
        "        '''\n",
        "        mode - train/valid/test\n",
        "        labels - list with all possible namelabels\n",
        "        transform - proccessing of file\n",
        "        image_shape - shape of result tensor\n",
        "        '''\n",
        "        \n",
        "        self.mode = mode\n",
        "        self.image_shape = image_shape\n",
        "        self.transform = transform if transform \\\n",
        "        else tf.Compose([tf.Resize(image_shape), tf.PILToTensor()])\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        self.device = device\n",
        "\n",
        "        self.check_mode = self.mode in ('train', 'valid')\n",
        "\n",
        "        self.le = LabelEncoder()\n",
        "        self.le.fit(labels)\n",
        "        # Saving tensors from PIL.Image\n",
        "        for path in files:\n",
        "            label = path.split('/')[-2]\n",
        "            tensor = self.get_sample(path)\n",
        "            self.x.append(tensor / 255)\n",
        "            self.y.append(label)\n",
        "\n",
        "        self._len = len(self.x)\n",
        "\n",
        "    def get_sample(self, filepath):\n",
        "        with PIL.Image.open(filepath) as image:\n",
        "            image = PIL.Image.open(filepath)\n",
        "            tensor = self.transform(image)\n",
        "        return tensor\n",
        "                    \n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        Returns Tensor, str (optional)\n",
        "        '''\n",
        "        if self.check_mode:\n",
        "            y = self.le.transform([self.y[idx]])\n",
        "            return self.x[idx], y[0]\n",
        "        else:\n",
        "            return self.x[idx]\n",
        "\n",
        "    def decode(self, num_label):\n",
        "        return self.le.inverse_transform([num_label])[0]\n",
        "\n",
        "    def train_valid_split(self, train_size = 0.9):\n",
        "        '''\n",
        "        Unfirom split of files.\n",
        "\n",
        "        Returns two datasets: train_dataset and valid_dataset\n",
        "        '''\n",
        "        def handle_one_class(label):\n",
        "            file_list = get_class_samples(label)\n",
        "            train_set, valid_set = train_test_split(tuple(file_list),\n",
        "                                                    train_size = train_size)\n",
        "            return train_set, valid_set\n",
        "\n",
        "        def get_class_samples(label):\n",
        "            return set([filename\n",
        "            for filename in self.files if label in filename.split('/')])\n",
        "\n",
        "        train_list = []\n",
        "        valid_list = []\n",
        "        labels = self.le.classes_\n",
        "        \n",
        "        for label in labels:\n",
        "            cur_train_list, cur_valid_list = handle_one_class(label)\n",
        "            train_list.extend(cur_train_list)\n",
        "            valid_list.extend(cur_valid_list)\n",
        "\n",
        "        train_ds = FastDataset(mode = 'train',\n",
        "                                      labels = labels,\n",
        "                                      image_shape = self.image_shape,\n",
        "                                      files = train_list)\n",
        "\n",
        "        valid_ds = FastDataset(mode = 'valid',\n",
        "                                      labels = labels,\n",
        "                                      image_shape = self.image_shape,\n",
        "                                      files = valid_list)\n",
        "        return train_ds, valid_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dyv0_7gbQyx"
      },
      "source": [
        "### CustomDataset(обработка, потом чтение)\n",
        "\n",
        "Это тоже класс датасета, но с медленным доступом. Здесь мы запоминаем все пути до картинок, потом при получении по индексу делаем предобработку, типа ресайз и перегоняем в тензор.\n",
        "\n",
        "+: Жрет немного памяти (ну во всяком случае меньше, чем `FastDataset`)\n",
        "\n",
        "+: Быстрая иницилазция\n",
        "\n",
        "-: Долго бегает"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeU6HdblouLU"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, mode = 'train', files = all_files, labels = labels,\n",
        "                 transform = None,\n",
        "                 image_shape = (200, 200)):\n",
        "        \n",
        "        '''\n",
        "        mode - train/valid/test\n",
        "        files - list/set with filepaths\n",
        "        labels - list with all possible namelabels\n",
        "        transform - proccessing of file\n",
        "        image_shape - shape of result tensor\n",
        "        '''\n",
        "\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        self.image_shape = image_shape\n",
        "        self.files = files\n",
        "        \n",
        "        self.check_mode = self.mode in ('train', 'test')\n",
        "        \n",
        "        self.le = LabelEncoder()\n",
        "        self.le.fit(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    \n",
        "    def default_transform(self, img):\n",
        "        '''\n",
        "        Make image resizing, and converting to tensor\n",
        "        '''\n",
        "        transform = tf.Compose([\n",
        "            tf.Resize(self.image_shape),\n",
        "            tf.PILToTensor()\n",
        "        ])\n",
        "        return transform(img)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.files[idx]\n",
        "        with PIL.Image.open(path) as img:\n",
        "            if self.transform:\n",
        "                tensor = self.transform(img)\n",
        "            else:\n",
        "                tensor = self.default_transform(img)\n",
        "\n",
        "        if self.check_mode:\n",
        "            label = self.get_label(idx)\n",
        "            return tensor, self.le.transform([label])[0]\n",
        "        else:\n",
        "            return tensor\n",
        "\n",
        "    def get_label(self, idx):\n",
        "        assert self.check_mode, \\\n",
        "        'It is not possible to get label'\n",
        "        path = self.files[idx]\n",
        "        return path.split('/')[2]\n",
        "\n",
        "    def decode(self, num_label):\n",
        "        return self.le.inverse_transform([num_label])[0]\n",
        "\n",
        "    def train_valid_split(self, train_size = 0.9):\n",
        "        '''\n",
        "        Unfirom split of files.\n",
        "\n",
        "        Returns two datasets: train_dataset and valid_dataset (augmentations = [None])\n",
        "        '''\n",
        "        def handle_one_class(label):\n",
        "            file_list = get_class_samples(label)\n",
        "            train_set, valid_set = train_test_split(tuple(file_list),\n",
        "                                                    train_size = train_size)\n",
        "            return train_set, valid_set\n",
        "\n",
        "        def get_class_samples(label):\n",
        "            return set([filename\n",
        "            for filename in self.files if label in filename.split('/')])\n",
        "\n",
        "        train_list = []\n",
        "        valid_list = []\n",
        "        labels = self.le.classes_\n",
        "        \n",
        "        for label in labels:\n",
        "            cur_train_list, cur_valid_list = handle_one_class(label)\n",
        "            train_list.extend(cur_train_list)\n",
        "            valid_list.extend(cur_valid_list)\n",
        "\n",
        "        train_ds = CustomDataset(mode = 'train',\n",
        "                                      labels = labels,\n",
        "                                      image_shape = self.image_shape,\n",
        "                                      files = train_list)\n",
        "        \n",
        "        valid_ds = CustomDataset(mode = 'valid',\n",
        "                                      labels = labels,\n",
        "                                      image_shape = self.image_shape,\n",
        "                                      files = valid_list)\n",
        "        return train_ds, valid_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q547p3nH9Llz"
      },
      "source": [
        "### `torchvision.datasets.DatasetFolder`\n",
        "Тоже прогоним через сравнение просто для проверки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jBfmsYLHlJA"
      },
      "outputs": [],
      "source": [
        "def make_DatasetFolder(path = init_path, transform = None,\n",
        "                       extensions = [], image_shape = (200, 200), mode = None,\n",
        "                       **kwargs):\n",
        "    def loader(path):\n",
        "        return PIL.Image.open(path)\n",
        "        \n",
        "    if not transform:\n",
        "        transform = tf.Compose([\n",
        "            tf.Resize(image_shape),\n",
        "            tf.PILToTensor()\n",
        "        ])\n",
        "\n",
        "    extensions = ['jpg', 'jpeg', 'png', 'webp']\n",
        "    return DatasetFolder(path, loader = loader,\n",
        "                         extensions = extensions, transform = transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhrc_O1H6wGT"
      },
      "source": [
        "### `AugmentedFastDataset`\n",
        "\n",
        "Версия `FastDataset`, дополненная аугментациями"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r2y-MkNdAyv"
      },
      "outputs": [],
      "source": [
        "class AugmentedFastDataset(Dataset):\n",
        "    def __init__(self, mode = 'train', files = all_files, labels = labels,\n",
        "                 transform = None,\n",
        "                 image_shape = (200, 200)):\n",
        "        '''\n",
        "        mode - train/valid/test\n",
        "        labels - list with all possible namelabels\n",
        "        transform - proccessing of file\n",
        "        image_shape - shape of result tensor\n",
        "        '''\n",
        "        \n",
        "        self.mode = mode\n",
        "        self.image_shape = image_shape\n",
        "        self.transform = transform if transform \\\n",
        "        else tf.Compose([tf.Resize(image_shape), tf.PILToTensor()])\n",
        "        \n",
        "        self.x = []\n",
        "        self.y = []\n",
        "\n",
        "        self.check_mode = self.mode in ('train', 'valid')\n",
        "        self._len = len(files)\n",
        "\n",
        "        self.le = LabelEncoder()\n",
        "        self.le.fit(labels)\n",
        "\n",
        "        self.augmentations = (\n",
        "            None, \n",
        "            tf.ColorJitter(brightness = 0.3,\n",
        "                           contrast = 0.3,\n",
        "                           saturation = 0.3),\n",
        "            tf.RandomPosterize(bits = 2, p = 1),\n",
        "            tf.RandomAdjustSharpness(sharpness_factor = 2,\n",
        "                                     p = 1),\n",
        "            tf.RandomEqualize(p = 1),\n",
        "            tf.RandomRotation(degrees = (-20, 20)),\n",
        "            tf.RandomHorizontalFlip(p = 1)\n",
        "        )\n",
        "\n",
        "        self.augmentations_amount = len(self.augmentations)\n",
        "\n",
        "        # Saving tensors from PIL.Image\n",
        "        for path in files:\n",
        "            label = path.split('/')[-2]\n",
        "            tensor = self.get_sample(path)\n",
        "            augmentations = self.get_augmented_samples(tensor)\n",
        "            self.x.extend(augmentations)\n",
        "            self.y.extend([label] * self.augmentations_amount)\n",
        "\n",
        "    def get_sample(self, filepath):\n",
        "        with PIL.Image.open(filepath) as image:\n",
        "            image = PIL.Image.open(filepath)\n",
        "            tensor = self.transform(image)\n",
        "        return tensor\n",
        "                    \n",
        "    def get_augmented_samples(self, tensor):\n",
        "        answer = [tensor / 255]\n",
        "        answer.extend(\n",
        "            [augmentation(tensor) / 255 \n",
        "            for augmentation in self.augmentations if augmentation]\n",
        "        )\n",
        "        return answer\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len * self.augmentations_amount\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        Returns Tensor, str (optional)\n",
        "        '''\n",
        "        if self.check_mode:\n",
        "            y = self.le.transform([self.y[idx]])\n",
        "            return self.x[idx], y[0]\n",
        "        else:\n",
        "            return self.x[idx]\n",
        "\n",
        "    def decode(self, num_label):\n",
        "        return self.le.inverse_transform([num_label])[0]\n",
        "\n",
        "    def train_valid_split(self, train_size = 0.9):\n",
        "        '''\n",
        "        Unfirom split of files.\n",
        "\n",
        "        Returns two datasets: train_dataset and valid_dataset (augmentations = [None])\n",
        "        '''\n",
        "        def handle_one_class(label):\n",
        "            file_list = get_class_samples(label)\n",
        "            train_set, valid_set = train_test_split(tuple(file_list),\n",
        "                                                    train_size = train_size)\n",
        "            return train_set, valid_set\n",
        "\n",
        "        def get_class_samples(label):\n",
        "            return set([filename\n",
        "            for filename in self.files if label in filename[0].split('/')])\n",
        "\n",
        "        train_list = []\n",
        "        valid_list = []\n",
        "        labels = self.le.classes_\n",
        "        \n",
        "        for label in labels:\n",
        "            cur_train_list, cur_valid_list = handle_one_class(label)\n",
        "            train_list.extend(cur_train_list)\n",
        "            valid_list.extend(cur_valid_list)\n",
        "\n",
        "        train_ds = AugmentedFastDataset(mode = 'train',\n",
        "                                      labels = labels,\n",
        "                                      image_shape = self.image_shape,\n",
        "                                      files = train_list,\n",
        "                                      augmentations = [None])\n",
        "        train_ds.augmentations = self.augmentations\n",
        "\n",
        "        valid_ds = AugmentedFastDataset(mode = 'valid',\n",
        "                                      labels = labels,\n",
        "                                      image_shape = self.image_shape,\n",
        "                                      files = valid_list,\n",
        "                                      augmentations = [None])\n",
        "        valid_ds.augmentations = self.augmentations\n",
        "        return train_ds, valid_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62u_hQUT7Dvb"
      },
      "source": [
        "### `AugemntedCustomDataset`\n",
        "\n",
        "Версия `CustomDataset`, дополненная аугментациями"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nDBuDhHfcLM"
      },
      "outputs": [],
      "source": [
        "class AugmentedCustomDataset(Dataset):\n",
        "    def __init__(self, mode = 'train', files = all_files, labels = labels,\n",
        "                 transform = None,\n",
        "                 image_shape = (200, 200), augmentations = None):\n",
        "        '''\n",
        "        mode - train/valid/test\n",
        "        files - list/set with filepaths\n",
        "        labels - list with all possible namelabels\n",
        "        transform - proccessing of file\n",
        "        image_shape - shape of result tensor\n",
        "        '''\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        self.image_shape = image_shape\n",
        "\n",
        "        self.check_mode = self.mode in ('train', 'valid')\n",
        "        \n",
        "        self.le = LabelEncoder()\n",
        "        self.le.fit(labels)\n",
        "\n",
        "        # Initialize augmentation options\n",
        "        if augmentations:\n",
        "            self.augmentations = augmentations\n",
        "        else:\n",
        "            self.augmentations = [\n",
        "                None, \n",
        "                tf.ColorJitter(brightness = 0.3,\n",
        "                            contrast = 0.3,\n",
        "                            saturation = 0.3),\n",
        "                tf.RandomPosterize(bits = 2, p = 1),\n",
        "                tf.RandomAdjustSharpness(sharpness_factor = 2,\n",
        "                                        p = 1),\n",
        "                tf.RandomEqualize(p = 1),\n",
        "                tf.RandomRotation(degrees = (-20, 20)),\n",
        "                tf.RandomHorizontalFlip(p = 1)\n",
        "            ]\n",
        "        self.augmentations_amount = len(self.augmentations)\n",
        "        if self.augmentations == [None]:\n",
        "            self.files = files\n",
        "        \n",
        "        else:\n",
        "            self.files = []\n",
        "            for filename in files:\n",
        "                augmented_filenames = [(filename, i) \n",
        "                                    for i in range(self.augmentations_amount)]\n",
        "                self.files.extend(augmented_filenames)\n",
        "\n",
        "        self._len = len(self.files)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "    \n",
        "    def default_transform(self, img):\n",
        "        '''\n",
        "        Make image resizing, and converting to tensor\n",
        "        '''\n",
        "        transform = tf.Compose([\n",
        "            tf.Resize(self.image_shape),\n",
        "            tf.PILToTensor()\n",
        "        ])\n",
        "        return transform(img)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Find path to file depending on idx\n",
        "        filename, augment_idx = self.files[idx]\n",
        "        augment = self.augmentations[augment_idx]\n",
        "        with PIL.Image.open(filename) as img:\n",
        "            if self.transform:\n",
        "                tensor = self.transform(img)\n",
        "            else:\n",
        "                tensor = self.default_transform(img)\n",
        "            \n",
        "            if augment:\n",
        "                tensor = augment(tensor)\n",
        "\n",
        "            tensor = tensor / 255\n",
        "\n",
        "        if self.check_mode:\n",
        "            label = self.get_label(filename)\n",
        "            return tensor, self.encode(label)\n",
        "        else:\n",
        "            return tensor\n",
        "\n",
        "    def get_label(self, path):\n",
        "        assert self.check_mode, \\\n",
        "        'It is not possible to get label'\n",
        "        return path.split('/')[-2]\n",
        "\n",
        "    def encode(self, str_label):\n",
        "        return self.le.transform([str_label])[0]\n",
        "\n",
        "    def decode(self, num_label):\n",
        "        return self.le.inverse_transform([num_label])[0]\n",
        "\n",
        "    def get_augmented_samples(self, idx):\n",
        "        begin_idx = idx * self.augmentations_amount\n",
        "        return [self[begin_idx + i][0] for i in range(self.augmentations_amount)]\n",
        "    \n",
        "    def draw_augmented_samples(self, idx):\n",
        "        samples = self.get_augmented_samples(idx)\n",
        "        plt.figure(figsize = (20, 20))\n",
        "        for i, sample in enumerate(samples):\n",
        "            plt.subplot(1, len(samples), i + 1)\n",
        "            plt.imshow(sample.permute(1, 2, 0))\n",
        "    \n",
        "    def analyze_splitting(self):\n",
        "        for_plot = {}\n",
        "        for filename in self.files:\n",
        "            label = self.get_label(filename)\n",
        "            if label in for_plot:\n",
        "                for_plot[label] += 1\n",
        "            else:\n",
        "                for_plot[label] = 1\n",
        "        for_plot = pd.DataFrame.from_dict(for_plot, orient = 'index',\n",
        "                                          columns = ['Amount'])\n",
        "        return for_plot\n",
        "\n",
        "    def train_valid_split(self, train_size = 0.9):\n",
        "        '''\n",
        "        Unfirom split of files.\n",
        "\n",
        "        Returns two datasets: train_dataset and valid_dataset (augmentations = [None])\n",
        "        '''\n",
        "        def handle_one_class(label):\n",
        "            file_list = get_class_samples(label)\n",
        "            train_set, valid_set = train_test_split(tuple(file_list),\n",
        "                                                    train_size = train_size)\n",
        "            return train_set, valid_set\n",
        "\n",
        "        def get_class_samples(label):\n",
        "            return set([filename\n",
        "            for filename in self.files if label in filename[0].split('/')])\n",
        "\n",
        "        train_list = []\n",
        "        valid_list = []\n",
        "        labels = self.le.classes_\n",
        "        \n",
        "        for label in labels:\n",
        "            cur_train_list, cur_valid_list = handle_one_class(label)\n",
        "            train_list.extend(cur_train_list)\n",
        "            valid_list.extend(cur_valid_list)\n",
        "\n",
        "        train_ds = AugmentedCustomDataset(mode = 'train',\n",
        "                                      labels = labels,\n",
        "                                      image_shape = self.image_shape,\n",
        "                                      files = train_list,\n",
        "                                      augmentations = [None])\n",
        "        train_ds.augmentations = self.augmentations\n",
        "\n",
        "        valid_ds = AugmentedCustomDataset(mode = 'valid',\n",
        "                                      labels = labels,\n",
        "                                      image_shape = self.image_shape,\n",
        "                                      files = valid_list,\n",
        "                                      augmentations = [None])\n",
        "        valid_ds.augmentations = self.augmentations\n",
        "        return train_ds, valid_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBBdnHiefeDU"
      },
      "source": [
        "### `AdvancedCustomDataset`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK7mSjJ2ftmL"
      },
      "source": [
        "В общем то, стало понятно, что обучение даже при использовании `AugmentedCustomDataset` не является эффективным, так как переобучение появляется уже на ранних этапах. Давайте доработаем `AugmentedCustomDataset` таким образом: теперь мы будем применять не фиксированный список возможных трансформаций, а будем дополнять уже существующее множество фотографий до определенного порога, и будем делать это для каждого класса. В результате работы планируется получить набор, который будет содержать одно и то же количество фотографий для каждого класса."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "P6tLb1YHfrRD"
      },
      "outputs": [],
      "source": [
        "class AdvancedCustomDataset(Dataset):\n",
        "    def __init__(self, augmentate, files = all_files, labels = labels, \n",
        "                 ex_amount = 1000, mode = 'train', transform = None,\n",
        "                 image_shape = (200, 200), augmentations = None):\n",
        "        '''\n",
        "        ex_amount - number of photo per class\n",
        "        mode - train/valid/test\n",
        "        files - list/set with filepaths\n",
        "        labels - list with all possible namelabels\n",
        "        transform - proccessing of file\n",
        "        image_shape - shape of result tensor\n",
        "        '''\n",
        "        self.mode = mode\n",
        "        self.transform = transform \\\n",
        "        if transform \\\n",
        "        else A.Compose([A.Normalize(),\n",
        "                        A.Resize(*image_shape)])\n",
        "        self.image_shape = image_shape\n",
        "        self.ex_amount = ex_amount\n",
        "        self.check_mode = self.mode in ('train', 'valid')\n",
        "        self.le = LabelEncoder()\n",
        "        self.le.fit(labels)\n",
        "\n",
        "        # Initialize augmentation options\n",
        "        if augmentations:\n",
        "            self.augmentations = augmentations\n",
        "        else:\n",
        "            self.augmentations = (\n",
        "                A.ColorJitter(brightness = 0.3,\n",
        "                              contrast = 0.3,\n",
        "                              saturation = 0.3),\n",
        "                A.Posterize(num_bits = 2, p = 1),\n",
        "                A.Sharpen(alpha = (0.9, 1.0)),\n",
        "                A.Equalize(p = 1),\n",
        "                A.Rotate(limit = (-20, 20), p = 1),\n",
        "                A.HorizontalFlip(p = 1)\n",
        "        )\n",
        "        self.augmentations_amount = len(self.augmentations)\n",
        "        self.files = files\n",
        "        if augmentate:\n",
        "            self.files = self.augmentate()\n",
        "\n",
        "        self._len = len(self.files)\n",
        "\n",
        "    def augmentate(self):\n",
        "        labels = self.le.classes_\n",
        "        new_files = []\n",
        "        for label in labels:\n",
        "            new_files_for_label = self.augmentate_one_class(label)\n",
        "            new_files.extend(new_files_for_label)\n",
        "        return new_files\n",
        "\n",
        "    def augmentate_one_class(self, label):\n",
        "        ex_amount = self.ex_amount\n",
        "        files = self.get_class_samples(label)\n",
        "        new_files = []\n",
        "        while len(new_files) < ex_amount:\n",
        "            filename = np.random.choice(files, size = 1)[0]\n",
        "            augmentations_amount = np.random.randint(low = 0,\n",
        "                                                     high = self.augmentations_amount)\n",
        "            if augmentations_amount:\n",
        "                augmentations = np.random.choice(a = self.augmentations,\n",
        "                                                 size = augmentations_amount,\n",
        "                                                 replace = False)\n",
        "                augmentations = A.Compose(augmentations)\n",
        "                new_files.append((filename, augmentations))\n",
        "            else:\n",
        "                new_files.append((filename, None))\n",
        "        return new_files\n",
        "\n",
        "    def get_class_samples(self, label):\n",
        "        return [filename\n",
        "        for filename in self.files if label in filename.split('/')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Find path to file depending on idx\n",
        "        filename, augmentations = self.files[idx]\n",
        "        img = cv2.imread(filename)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        tensor = self.transform(image = img)['image']\n",
        "        if augmentations:\n",
        "            tensor = augmentations(image = tensor)['image']\n",
        "\n",
        "        tensor = tensor / 255\n",
        "        tensor = Ap.ToTensorV2()(image = tensor)['image'].float()\n",
        "\n",
        "        if self.check_mode:\n",
        "            label = self.get_label(filename)\n",
        "            return tensor, self.encode(label)\n",
        "        else:\n",
        "            return tensor\n",
        "\n",
        "    def get_augmented_samples(self, idx):\n",
        "        '''\n",
        "        Method to get all augmentations with the same image\n",
        "        idx - index in self.files\n",
        "        '''\n",
        "        filename = self.files[idx][0]\n",
        "        answer = [item for item in self.files\n",
        "                  if filename == item[0]]\n",
        "        return answer\n",
        "\n",
        "    def draw_augmented_samples(self, idx):\n",
        "        files = self.get_augmented_samples(idx)\n",
        "        columns = 5\n",
        "        number = len(files)\n",
        "        if number % columns:\n",
        "            lines = int(number / columns) + 1\n",
        "        else:\n",
        "            lines = int(number / columns)\n",
        "        print(f'{number}: {lines}:{columns}')\n",
        "        plt.figure(figsize = (20, 20))\n",
        "        for idx, item in enumerate(files):\n",
        "            filename, augmentation = item\n",
        "            img = cv2.imread(filename)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            if augmentation:\n",
        "                img = augmentation(image = img)['image']\n",
        "            plt.subplot(lines, columns, idx + 1)\n",
        "            plt.imshow(img)\n",
        "\n",
        "    def get_label(self, path):\n",
        "        assert self.check_mode, \\\n",
        "        'It is not possible to get label'\n",
        "        return path.split('/')[-2]\n",
        "\n",
        "    def encode(self, str_label):\n",
        "        return self.le.transform([str_label])[0]\n",
        "\n",
        "    def decode(self, num_label):\n",
        "        return self.le.inverse_transform([num_label])[0]\n",
        "\n",
        "    def train_valid_split(self, train_size = 0.9):\n",
        "        '''\n",
        "        Unfirom split of files.\n",
        "\n",
        "        Returns two datasets: train_dataset and valid_dataset\n",
        "        '''\n",
        "        def handle_one_class(label):\n",
        "            file_list = get_class_samples(label)\n",
        "            train_set, valid_set = train_test_split(tuple(file_list),\n",
        "                                                    train_size = train_size)\n",
        "            return train_set, valid_set\n",
        "\n",
        "        def get_class_samples(label):\n",
        "            return set([filename\n",
        "            for filename in self.files if label in filename[0].split('/')])\n",
        "\n",
        "        train_list = []\n",
        "        valid_list = []\n",
        "        labels = self.le.classes_\n",
        "        \n",
        "        for label in labels:\n",
        "            cur_train_list, cur_valid_list = handle_one_class(label)\n",
        "            train_list.extend(cur_train_list)\n",
        "            valid_list.extend(cur_valid_list)\n",
        "\n",
        "        train_ds = AdvancedCustomDataset(augmentate = False, mode = 'train',\n",
        "                                      labels = labels,\n",
        "                                      image_shape = self.image_shape,\n",
        "                                      files = train_list)\n",
        "        train_ds.augmentations = self.augmentations\n",
        "\n",
        "        valid_ds = AdvancedCustomDataset(augmentate = False, mode = 'valid',\n",
        "                                      labels = labels,\n",
        "                                      image_shape = self.image_shape,\n",
        "                                      files = valid_list)\n",
        "        valid_ds.augmentations = self.augmentations\n",
        "        return train_ds, valid_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tPpz2ClN7wH"
      },
      "source": [
        "### Сравнение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zcvuJ9EF7Tv"
      },
      "outputs": [],
      "source": [
        "def memory_counter(ex, all = False):\n",
        "    '''\n",
        "    Memory counter for existing class instance\n",
        "    all - count all variables and methods in ex, else exclude __methods__\n",
        "    '''\n",
        "    mem = 0\n",
        "    if all:\n",
        "        for key, val in ex.__dict__.items():\n",
        "            mem += sys.getsizeof(val)\n",
        "        return mem\n",
        "    else:\n",
        "        for key, val in ex.__dict__.items():\n",
        "            if key.startswith('_'):\n",
        "                continue\n",
        "            else:\n",
        "                mem += sys.getsizeof(val)\n",
        "        return mem\n",
        "\n",
        "def dataset_metric(cls, print_info = True, **kwargs):\n",
        "    '''\n",
        "    Comparing of classes with datasets: init, traverse, memory\n",
        "    '''\n",
        "    print(f'Class name: {cls.__name__}')\n",
        "    begin = time.time()\n",
        "    ex = cls(**kwargs)\n",
        "    to_init = time.time() - begin\n",
        "    print('Time to init: {:.5f} s'.format(to_init))\n",
        "    begin = time.time()\n",
        "    for _ in ex:\n",
        "        pass\n",
        "    to_traverse = time.time() - begin\n",
        "    print('Time to traverse: {:.5f} s'.format(to_traverse))\n",
        "    memory = memory_counter(ex)\n",
        "    info = '\\n'.join(['Memory: {} bytes = {:.3f} MB',\n",
        "                    'Total elements: {} elements',\n",
        "                    'Mean iteration time: {:.4f} s',\n",
        "                    'Mean memory usage per element: {:.4f} bytes',\n",
        "                     '']).format(memory, memory / 10 ** 6, \n",
        "                                 len(ex), \n",
        "                                 to_traverse / len(ex),\n",
        "                                 memory / len(ex))\n",
        "    if print_info:\n",
        "        print(info)\n",
        "    \n",
        "    d = (cls.__name__, to_init, to_traverse, memory)\n",
        "    del ex\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuD_S8CGNiyn",
        "outputId": "0f6d8038-a724-4f2a-922b-ed29081fb1a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class name: AdvancedCustomDataset\n",
            "Time to init: 22.53049 s\n",
            "Time to traverse: 1930.73612 s\n",
            "Memory: 1764466 bytes = 1.764 MB\n",
            "Total elements: 210000 elements\n",
            "Mean iteration time: 0.0092 s\n",
            "Mean memory usage per element: 8.4022 bytes\n",
            "\n"
          ]
        }
      ],
      "source": [
        "advds_metric = dataset_metric(AdvancedCustomDataset, augmentate = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXEd3l6hPayL",
        "outputId": "65b99c19-91ae-492c-92b9-399667a8434d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class name: CustomDataset\n",
            "Time to init: 0.00100 s\n",
            "Time to traverse: 126.87075 s\n",
            "Memory: 84378 bytes = 0.084 MB\n",
            "Total elements: 10515 elements\n",
            "Mean iteration time: 0.0121 s\n",
            "Mean memory usage per element: 8.0245 bytes\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cds_metric = dataset_metric(CustomDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBiLKY-A0YX7",
        "outputId": "76de6c21-93ed-4e00-a0a7-5e452b8a2bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class name: FastDataset\n",
            "Time to init: 131.19805 s\n",
            "Time to traverse: 3.40911 s\n",
            "Memory: 170610 bytes = 0.171 MB\n",
            "Total elements: 10515 elements\n",
            "Mean iteration time: 0.0003 s\n",
            "Mean memory usage per element: 16.2254 bytes\n",
            "\n"
          ]
        }
      ],
      "source": [
        "fds_metric = dataset_metric(FastDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOkRJxsv0dp7",
        "outputId": "f4abb65d-8852-42c1-d0a9-97dd82c7cb3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class name: make_DatasetFolder\n",
            "Time to init: 0.09396147727966309\n",
            "Time to traverse: 127.35288572311401\n",
            "Memory: 2409920 bytes = 2.410 MB\n",
            "Total elements: 10534 elements\n",
            "Mean iteration time: 0.012\n",
            "Mean memory per element: 228.775 bytes\n"
          ]
        }
      ],
      "source": [
        "dfds_metric = dataset_metric(make_DatasetFolder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xNRbcfM7gy_",
        "outputId": "d05e6566-9124-4ea7-fdbd-487b2ce75fa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class name: AugmentedCustomDataset\n",
            "Time to init: 0.020332813262939453\n",
            "Time to traverse: 1068.542881011963\n",
            "Memory: 6457296 bytes = 6.457 MB\n",
            "Total elements: 73605 elements\n",
            "Mean iteration time: 0.015\n",
            "Mean memory per element: 87.729 bytes\n"
          ]
        }
      ],
      "source": [
        "acds_metric = dataset_metric(AugmentedCustomDataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mns_79Kqatr6"
      },
      "source": [
        "### Выводы\n",
        "Тут надо подумать и выбрать, пока буду юзать Fast.\n",
        "\n",
        "Стоит еще заметить вот что: при работе с `torchvision.datasets.DatasetFolder` у нас классификация происходит иначе, нежели в остальных классах. Соответственно, если обучить сетку, а потом поменять тип используемого датасета, то будет плохо. Поэтому на этом мы прощаемся этой штукой. Не очень грустно, потому что в сравнении с другими вариантами она не сказать что превосходит по времени/памяти.\n",
        "\n",
        "В перспективе, конечно, правильнее будет использовать `CustomDataset`, потому что при расширении датасета для `FastDataset` может банально не хватить памяти. Пока же, мы будем использовать `FastDataset`, и если что, добавлю возможность переключения на `CustomDataset`.\n",
        "\n",
        "Но как выяснилось, данных слишком мало, поэтому их пришлось аугментировать. Создал два новых класса: `AugmentedCustomDataset` - аналог `CustomDataset` с возможностью аугментации, и `AugmentedFastDataset` - аналог `FastDataset` с возможностью аугментации.\n",
        "\n",
        "Далее, будем использовать `AugmentedCustomDataset`, потому что быстрый аналог банально перестал влезать в память. Но придется потерять во времени..("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrEARZntOa-v"
      },
      "source": [
        "Также, во время прогонки `AdvancedCustomDataset` выяснилось, что методы аугментации из `torchvision.transforms` работают несколько дольше, нежели аналоги из `albumentations`. Поэтому в `AdvancedCustomDataset` будут использоваться методы модуля `albumentations`.\n",
        "\n",
        "А еще, оказалось, что считывание через `cv2.imread` работает быстрее,чем `PIL.Image.open`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS1ysaaPZdkh"
      },
      "source": [
        "## Разбиение данных\n",
        "\n",
        "Это я выделил в отдельный раздельчик, причина, как по мне, существенная: картинок очень мало (по 50 на класс), классов очень много (210) и хотелось бы проконтроллировать, чтоб в тренировочной выборке был баланс классов. Я думаю, что в тренировочную выборку мы закинем 90% (для начала, дальше видно будет). Возможно, придется обучать на всей выборке, а потом валидиться на каком то подмножестве тренировочной выборки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sl2RxSztsh3n"
      },
      "outputs": [],
      "source": [
        "def make_loaders(ds_cls, train_size, train_bs, valid_bs, ds_params):\n",
        "    '''\n",
        "    ds_cls - class of using dataset\n",
        "    Return two DataLoaders: train and valid\n",
        "    '''\n",
        "\n",
        "    ds = ds_cls(**ds_params)\n",
        "    train_ds, valid_ds = ds.train_valid_split(train_size = train_size)\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size = train_bs,\n",
        "                          shuffle = True, num_workers = 1)\n",
        "\n",
        "    valid_dl = DataLoader(valid_ds, batch_size = valid_bs,\n",
        "                          shuffle = False, num_workers = 1)\n",
        "    \n",
        "    return train_dl, valid_dl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wGXQ3ejatRj"
      },
      "source": [
        "## Цикл обучения с валидацией"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yDnzHsDOwAT9"
      },
      "outputs": [],
      "source": [
        "def train_valid(model, train_dl, valid_dl,\n",
        "                opt_cls, opt_params, loss_fn, \n",
        "                metric_fn, max_epochs:int,\n",
        "                device, exp_name,\n",
        "                scheduler_cls = None, scheduler_params = None):\n",
        "    '''\n",
        "    Train and validation cycle.\n",
        "    \n",
        "    train_dl - DataLoader with train data\n",
        "    valid_dl - DataLoader with valid data\n",
        "    opt - optimizer\n",
        "    loss_fn - loss function\n",
        "    metric_fn - metric function to evaluate model\n",
        "    max_epochs - epochs to training and validation\n",
        "    scheduler_cls - class of scheduler\n",
        "    '''\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    train_metric = []\n",
        "    valid_metric = []\n",
        "    \n",
        "    def print_loss_metric_info(train_loss = train_losses, \n",
        "                               valid_loss = valid_losses,\n",
        "                               train_metric = train_metric, \n",
        "                               valid_metric = valid_metric):\n",
        "        '''\n",
        "        Logger function\n",
        "        '''\n",
        "        template = '\\n'.join(['',\n",
        "                              'Losses on train: {}',\n",
        "                              'Losses on valid: {}',\n",
        "                              'Metric on train: {}',\n",
        "                              'Metric on valid: {}'])\n",
        "        print(template.format(train_loss,\n",
        "                               valid_loss,\n",
        "                               train_metric,\n",
        "                               valid_metric))\n",
        "\n",
        "    # Optimizer initialization\n",
        "    opt = opt_cls(params = model.parameters(), \n",
        "                  **opt_params)\n",
        "    \n",
        "    # Scheduler initialization\n",
        "    if scheduler_cls:\n",
        "        scheduler = scheduler_cls(optimizer = opt,\n",
        "                                  **scheduler_params)\n",
        "    else:\n",
        "        scheduler = None\n",
        "    model = model.to(device)\n",
        "    train_time = 0\n",
        "    valid_time = 0\n",
        "    for epoch in tqdm(range(max_epochs), desc = 'Epoch'):\n",
        "    # Training cycle\n",
        "        model.train()\n",
        "        train_losses_epoch = []\n",
        "        train_metric_epoch = []\n",
        "        print_loss_metric_info()\n",
        "        begin_time = time.time()\n",
        "        for x, y in tqdm(train_dl):\n",
        "            opt.zero_grad()\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            output = model(x)\n",
        "            y_pred = torch.argmax(output, dim = -1)\n",
        "\n",
        "            loss = loss_fn(output, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            metric_value = metric_fn(y.to('cpu'), y_pred.to('cpu'), average = 'macro')\n",
        "            train_metric_epoch.append(metric_value)\n",
        "            train_losses_epoch.append(loss.item())\n",
        "        train_time += (time.time() - begin_time)\n",
        "        train_losses.append(np.mean(train_losses_epoch))\n",
        "        train_metric.append(np.mean(train_metric_epoch))\n",
        "\n",
        "    # Valid cycle\n",
        "        model.eval()\n",
        "        valid_losses_epoch = []\n",
        "        valid_metric_epoch = []\n",
        "        print_loss_metric_info()\n",
        "        begin_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            for x, y in tqdm(valid_dl):\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                output = model(x)\n",
        "                y_pred = torch.argmax(output, dim = -1)\n",
        "\n",
        "                loss = loss_fn(output, y)\n",
        "\n",
        "                metric_value = metric_fn(y.to('cpu'), y_pred.to('cpu'), average = 'macro')\n",
        "                valid_losses_epoch.append(loss.item())\n",
        "                valid_metric_epoch.append(metric_value)\n",
        "\n",
        "        valid_metric.append(np.mean(valid_metric_epoch))\n",
        "        valid_losses.append(np.mean(valid_losses_epoch))\n",
        "        valid_time += (time.time() - begin_time)\n",
        "        # Saving model params\n",
        "        if valid_metric[-1] == max(valid_metric):\n",
        "            torch.save(model.state_dict(),\n",
        "                './models/models/' + exp_name + '.pth')\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        clear_output()\n",
        "\n",
        "    return train_losses, valid_losses, train_metric, valid_metric, train_time, valid_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwn8hIogbgxH"
      },
      "source": [
        "## Написание моделей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gXwH2kwbncZ"
      },
      "source": [
        "В качестве моделей мы будем использовать сверточные нейронные сети и различные ансамбли."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7GbaDHtpIWC"
      },
      "source": [
        "### Сверточные сети\n",
        "\n",
        "Для перебора архитектур не будем заводить отдельного класса, а напишем один раз шаблон и будем его менять прям в коде, потому что далее все равно будет выполняться сохранение моделей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNHVGjg0ziL9"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, n_classes = len(labels)):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 8, 7)\n",
        "        self.c_act1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(8, 32, 3)\n",
        "        self.c_act2 = nn.ReLU()\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
        "        self.c_act3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        \n",
        "        self.conv4 = nn.Conv2d(64, 128, 3)\n",
        "        self.c_act4 = nn.ReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(128, 256, 3)\n",
        "        self.c_act5 = nn.ReLU() \n",
        "        self.pool5 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv6 = nn.Conv2d(256, 256, 3)\n",
        "        self.c_act6 = nn.ReLU()\n",
        "\n",
        "        self.flattener = nn.Flatten()\n",
        "\n",
        "        self.bn3 = nn.BatchNorm1d(9216)\n",
        "\n",
        "        self.linear1 = nn.Linear(9216, 4096)\n",
        "        self.l_act1 = nn.ReLU()\n",
        "\n",
        "        self.linear2 = nn.Linear(4096, 1024)\n",
        "        self.l_act2 = nn.ReLU()\n",
        "\n",
        "        self.linear3 = nn.Linear(1024, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.c_act1(self.conv1(x)))\n",
        "        \n",
        "        x = self.c_act2(self.conv2(x))\n",
        "        x = self.bn1(x)\n",
        "        \n",
        "        x = self.pool3(self.c_act3(self.conv3(x)))\n",
        "        \n",
        "        x = self.c_act4(self.conv4(x))\n",
        "        x = self.bn2(x)\n",
        "        \n",
        "        x = self.pool5(self.c_act5(self.conv5(x)))\n",
        "        \n",
        "        x = self.c_act6(self.conv6(x))\n",
        "\n",
        "        x = self.flattener(x)\n",
        "        x = self.bn3(x)\n",
        "        \n",
        "        x = self.l_act1(self.linear1(x))\n",
        "        \n",
        "        x = self.l_act2(self.linear2(x))\n",
        "        \n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eNsTMHN5MA2i"
      },
      "outputs": [],
      "source": [
        "def train_valid_save(model, artifact_config,\n",
        "                     preprocess_config,\n",
        "                     train_config):\n",
        "    \n",
        "    art = wandb.Artifact(**artifact_config)\n",
        "    exp_name = artifact_config['name']\n",
        "    print('Making dataloaders...')\n",
        "    train_dl, valid_dl = make_loaders(**preprocess_config)\n",
        "    clear_output()\n",
        "    train_losses, valid_losses, train_metric, valid_metric, train_time, valid_time = train_valid(\n",
        "        model = model,\n",
        "        train_dl = train_dl,\n",
        "        valid_dl = valid_dl,\n",
        "        exp_name = exp_name,\n",
        "        **train_config\n",
        "    )\n",
        "\n",
        "    epochs = train_config['max_epochs']\n",
        "    for_table = list(zip(range(1, epochs + 1), \n",
        "                         train_losses,\n",
        "                         valid_losses,\n",
        "                         train_metric,\n",
        "                         valid_metric)) \n",
        "    \n",
        "    tabled_cfg = wandb.Table(\n",
        "        columns = ['Epoch', 'Train losses', 'Valid losses', 'Train score', 'Valid score'],\n",
        "        data = for_table\n",
        "    )\n",
        "\n",
        "    # Model state dict\n",
        "    art.add_file('./models/models/' + exp_name + '.pth',\n",
        "                 name = 'state_dict.pth')\n",
        "    \n",
        "    # Losses and metrics\n",
        "    art.add(tabled_cfg, 'Losses and scores table')\n",
        "\n",
        "    # Add result description\n",
        "    result_config = {'Train time': train_time,\n",
        "                     'Valid time': valid_time,\n",
        "                     'Device': device}\n",
        "\n",
        "    # Add configuration\n",
        "    common_config = {'Preprocess': preprocess_config,\n",
        "                     'Training': train_config,\n",
        "                     'Resulting': result_config}\n",
        "\n",
        "    art.metadata = common_config\n",
        "\n",
        "    x = next(model.modules())\n",
        "    with open('./models/desc/' + exp_name + '.txt', 'w') as f:\n",
        "        f.write(str(x))\n",
        "\n",
        "    art.add_file('./models/desc/' + exp_name + '.txt',\n",
        "                 name = 'desc.txt')\n",
        "\n",
        "    wandb.log_artifact(art)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Lh802qOeGvRi"
      },
      "outputs": [],
      "source": [
        "dataset_config = {\n",
        "    'augmentate': True,\n",
        "    'ex_amount': 2000,\n",
        "    'image_shape': (224, 224)\n",
        "}\n",
        "\n",
        "\n",
        "preprocess_config = {\n",
        "    'ds_cls': AdvancedCustomDataset,\n",
        "    'ds_params': dataset_config,\n",
        "    'train_bs': 128,\n",
        "    'valid_bs': 256,\n",
        "    'train_size': 0.9\n",
        "}\n",
        "\n",
        "train_config = {\n",
        "    'opt_cls': torch.optim.Adam,\n",
        "    'loss_fn': nn.CrossEntropyLoss(),\n",
        "    'metric_fn': f1_score,\n",
        "    'max_epochs': 10,\n",
        "    'opt_params': {\n",
        "        'lr': 5e-4\n",
        "    },\n",
        "    'scheduler_cls': torch.optim.lr_scheduler.StepLR,\n",
        "    'scheduler_params': {\n",
        "        'step_size': 4,\n",
        "        'gamma': 0.55\n",
        "    },\n",
        "    'device': device\n",
        "}\n",
        "\n",
        "artifact_config = {\n",
        "    'name': 'CNN_v.1',\n",
        "    'type': 'model',\n",
        "    'description': \n",
        "    '''Using advanced dataset;\n",
        "    '''\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "aI70rjr9QrjM",
        "outputId": "9ab0bafa-62d0-45de-aeeb-a6fc23b16022"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-204d7479aa8f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_valid_save(model = model,\n\u001b[0m\u001b[1;32m      2\u001b[0m                  \u001b[0martifact_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0martifact_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                  \u001b[0mpreprocess_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  train_config = train_config)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_valid_save' is not defined"
          ]
        }
      ],
      "source": [
        "train_valid_save(model = model,\n",
        "                 artifact_config = artifact_config,\n",
        "                 preprocess_config = preprocess_config,\n",
        "                 train_config = train_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6cBA_X_5yh0",
        "outputId": "bc5bc5e3-740f-441d-bb85-295209efdebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Apr 20 20:42:29 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "mpHbAJysMGPv",
        "outputId": "2e4e3988-27ae-4077-cf47-c5dfdd15a827"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">deft-cloud-20</strong> at: <a href='https://wandb.ai/ml_landmarks/ml_landmarks/runs/3aqvai3b' target=\"_blank\">https://wandb.ai/ml_landmarks/ml_landmarks/runs/3aqvai3b</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230420_164549-3aqvai3b/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuP8McSV9fvi"
      },
      "source": [
        "Я пытался поменять способ рейшейпинга, но время обучения увеличилось в 2.5 раза.\n",
        "Возможно, что это из-за weight decay.\n",
        "\n",
        "Добавить батчнорм\n",
        "\n",
        "ArcFace?\n",
        "\n",
        "Лоссы вообще посмотреть\n",
        "\n",
        "Гугл датасет сохранил на кагл\n",
        "\n",
        "Файнтьюн?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyfSg5xEMtLF"
      },
      "source": [
        "## Файнтьюн"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG13"
      ],
      "metadata": {
        "id": "2IDb3A2LKiAq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCrEaVUcMsiE",
        "outputId": "8ed1a25f-4a33-42ee-93b4-e748874ec03b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG13_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG13_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\" to /root/.cache/torch/hub/checkpoints/vgg13_bn-abd245e5.pth\n",
            "100%|██████████| 508M/508M [00:03<00:00, 167MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "vgg13 = models.vgg13_bn(weights = models.VGG13_BN_Weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "A93L3X_HQBY_"
      },
      "outputs": [],
      "source": [
        "for x in vgg13.features.parameters():\n",
        "    x.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "om8rV_YTize3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96f651cd-8492-4d8e-f960-8ec983b935f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (16): ReLU(inplace=True)\n",
              "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (21): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (23): ReLU(inplace=True)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (30): ReLU(inplace=True)\n",
              "    (31): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (32): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (33): ReLU(inplace=True)\n",
              "    (34): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=1024, out_features=210, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "vgg13.classifier[-4] = nn.Linear(in_features = 4096,\n",
        "                                 out_features = 1024)\n",
        "vgg13.classifier[-1] = nn.Linear(in_features = 1024,\n",
        "                                 out_features = len(labels))\n",
        "vgg13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "rScaQZcBlA2C",
        "outputId": "3e00737a-cf35-4d87-d888-adfac9474be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Losses on train: []\n",
            "Losses on valid: []\n",
            "Metric on train: []\n",
            "Metric on valid: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/2537 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 1/2537 [01:40<70:57:43, 100.73s/it]\n",
            "Epoch:   0%|          | 0/10 [01:40<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-41da5b33326c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpreprocess_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_losses, valid_losses, train_metric, valid_metric, train_time, valid_time = train_valid(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalid_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-54a67e132ddd>\u001b[0m in \u001b[0;36mtrain_valid\u001b[0;34m(model, train_dl, valid_dl, opt_cls, opt_params, loss_fn, metric_fn, max_epochs, device, exp_name, scheduler_cls, scheduler_params)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_dl, valid_dl = make_loaders(**preprocess_config)\n",
        "train_losses, valid_losses, train_metric, valid_metric, train_time, valid_time = train_valid(\n",
        "        model = vgg13,\n",
        "        train_dl = train_dl,\n",
        "        valid_dl = valid_dl,\n",
        "        exp_name = 'Vgg13',\n",
        "        **train_config\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resnet"
      ],
      "metadata": {
        "id": "wg-Ybrd3Kj-4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SGMCtIqshQW"
      },
      "outputs": [],
      "source": [
        "resnet = models.resnet50(weights = models.ResNet50_Weights)\n",
        "resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMjX222vsunK",
        "outputId": "6342ef1c-5a2b-4e43-de13-17e25cb20583"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=210, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for param in resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "resnet.fc = nn.Linear(in_features = 2048,\n",
        "                      out_features = len(labels))\n",
        "resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0sgb9td3PEJ"
      },
      "outputs": [],
      "source": [
        "densenet = models.de"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD8IxRDrZNqC"
      },
      "source": [
        "# TODO:\n",
        "<font color='red'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GILf8neHv0gE"
      },
      "source": [
        "## Ускорение инференса и обучение\n",
        "\n",
        "*    Добавить батчнорм\n",
        "*    Прунинг (надо чтоб она хоть какое то качество показала)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7XNEgDVwbTA"
      },
      "source": [
        "## Интерактивное создание сетей\n",
        "\n",
        "Тут я пытался сделать небольшую утилку, которая всякими слайдерами и дропдаунами могла бы генерить слои нейронной сети. Просто по приколу."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OlIrXxx3uIe"
      },
      "outputs": [],
      "source": [
        "def selector(**kwargs):\n",
        "\n",
        "    def conv(**kwargs):\n",
        "        return nn.Conv2d(**kwargs)\n",
        "    \n",
        "    def linear(**kwargs):\n",
        "        return nn.Linear(**kwargs)\n",
        "\n",
        "    def activation(**kwargs):\n",
        "        cls = kwargs['cls']\n",
        "        if cls == 'ReLU':\n",
        "            return nn.ReLU()\n",
        "        if cls == 'Tanh':\n",
        "            return nn.Tanh()\n",
        "        if cls == 'Sigmoid':\n",
        "            return nn.Sigmoid()\n",
        "    \n",
        "    def dropout(**kwargs):\n",
        "        return nn.Dropout1d(**kwargs)\n",
        "\n",
        "    def pooling(**kwargs):\n",
        "        pool_type = kwargs['pool_type']\n",
        "        del kwargs['pool_type']\n",
        "        if pool_type == 'max':\n",
        "            return nn.MaxPool2d(**kwargs)\n",
        "    \n",
        "    layer = kwargs['layer_type']\n",
        "    if layer == 'Conv2d':\n",
        "        w = widgets.interact_manual(conv,\n",
        "                                    in_channels = widgets.IntText(value = 1),\n",
        "                                   out_channels = widgets.IntText(value = 1),\n",
        "                                   kernel_size = widgets.IntText(value = 1),\n",
        "                                   padding = widgets.IntText(value = 0),\n",
        "                                   stride = widgets.IntText(value = 1))\n",
        "    \n",
        "    elif layer == 'Linear':\n",
        "        w = widgets.interactive(linear, {'manual': True, 'auto_display': True},\n",
        "                                   in_features = widgets.IntText(value = 1),\n",
        "                                   out_features = widgets.IntText(value = 1))\n",
        "        print(f'Linear {w}')\n",
        "\n",
        "    elif layer == 'Activation':\n",
        "        w = widgets.interactive(activation, \n",
        "                                {'manual': True, 'auto_display': True},\n",
        "                                cls = ['ReLU', 'Tanh', 'Sigmoid'])\n",
        "        print(f'ww - {w}')\n",
        "\n",
        "    elif layer == 'Dropout':\n",
        "        w = widgets.interact_manual(dropout,\n",
        "                                   p = widgets.FloatText(value = 0.5,\n",
        "                                                         min = 0,\n",
        "                                                         max = 1))\n",
        "    \n",
        "    elif layer == 'Pooling':\n",
        "        w =  widgets.interact_manual(pooling,\n",
        "                                 pool_type = ['max', 'avg', 'min'],\n",
        "                                 kernel_size = widgets.IntText(value = 1),\n",
        "                                 padding = widgets.IntText(value = 0),\n",
        "                                 stride = widgets.IntText(value = 1))\n",
        "        \n",
        "    display(w)\n",
        "    print(f'w = {w}')\n",
        "    return w\n",
        "\n",
        "\n",
        "result = widgets.interactive(selector, layer_type = ['Conv2d',\n",
        "                                                  'Pooling',\n",
        "                                                  'Linear',\n",
        "                                                  'Activation',\n",
        "                                                  'Dropout'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "j7inGNDvBlb7",
        "outputId": "27f3d26e-6b44-4a0a-ff5b-ea0698582d9d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2a8ea8d0e784f809f0e1d3418abbd10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='layer_type', options=('Conv2d', 'Pooling', 'Linear', 'Activation',…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z = (Dropdown(description='layer_type', options=('Conv2d', 'Pooling', 'Linear', 'Activation', 'Dropout'), value='Conv2d'), Output())\n"
          ]
        }
      ],
      "source": [
        "display(result)\n",
        "z = result.children\n",
        "print(f'z = {z}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGoydRmTeasL"
      },
      "outputs": [],
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    _number = 1\n",
        "    def __init__(self, conv_params, conv_cls = nn.Conv2d,\n",
        "                 pooling_cls = None, pooling_params = None,\n",
        "                 activation_cls = None, activation_params = None):\n",
        "        '''\n",
        "        Create conv layer: conv2d->pooling->activation\n",
        "        *_params - dict with layer params\n",
        "        *_cls - class of layer\n",
        "        '''\n",
        "        number = ConvLayer._number\n",
        "        super().__init__()\n",
        "        conv_layer = conv_cls(**conv_params)\n",
        "        self.conv_layer = nn.Sequential()\n",
        "        self.conv_layer.add_module(f'Conv_{number}', conv_layer)\n",
        "\n",
        "        if pooling_cls:\n",
        "            pooling_layer = pooling_cls(**pooling_params)\n",
        "            self.conv_layer.add_module(f'Pooling_{number}', pooling_layer)\n",
        "\n",
        "        if activation_cls:\n",
        "            activation = activation_cls(**activation_params)\n",
        "            self.conv_layer.add_module(f'Activation_{number}', activation)\n",
        "\n",
        "        ConvLayer._number += 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_layer(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8pcv-2Pp5Eo"
      },
      "outputs": [],
      "source": [
        "class FCLayer(nn.Module):\n",
        "    _number = 1\n",
        "    def __init__(self, linear_params, linear_cls = nn.Linear,\n",
        "                 dropout_cls = None, dropout_params = None,\n",
        "                 activation_cls = None, activation_params = None):\n",
        "        '''\n",
        "        Create FC-layer: dropout->linear->activation\n",
        "        If current layer is last, actvation can be replaced on\n",
        "        something like Softmax, etc.\n",
        "        '''\n",
        "\n",
        "        super().__init__()\n",
        "        self.fc_layer = nn.Sequential()\n",
        "        number = FCLayer._number\n",
        "        if dropout_cls:\n",
        "            dropout_layer = dropout_cls(**dropout_params)\n",
        "            self.fc_layer.add_module(f'Dropout_{number}', dropout_layer)\n",
        "        \n",
        "        linear = linear_cls(**linear_params)\n",
        "        self.fc_layer.add_module(f'Linear_{number}', linear)\n",
        "        \n",
        "        if activation_cls:\n",
        "            if activation_params:\n",
        "                activation = activation_cls(**activation_params)\n",
        "            else:\n",
        "                activation = activation_cls()\n",
        "            self.fc_layer.add_module(f'Activation_{number}', activation)\n",
        "\n",
        "        FCLayer._number += 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc_layer(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KU5V8oC7jpDm"
      },
      "outputs": [],
      "source": [
        "class MiddleLayer(nn.Module):\n",
        "    _number = 1\n",
        "    def __init__(self, params = None):\n",
        "        '''\n",
        "        Create layers with middle layers\n",
        "        params - tuple (layer_cls, dict(layer_params))\n",
        "        '''\n",
        "        super().__init__()\n",
        "        number = MiddleLayer._number\n",
        "        if isinstance(params, (tuple, list)):\n",
        "            self.mid_layer = nn.Sequential()\n",
        "            for elem in params:\n",
        "                cls, cls_params = elem\n",
        "                if cls_params:\n",
        "                    layer = cls(**cls_params)\n",
        "                else:\n",
        "                    layer = cls()\n",
        "\n",
        "                self.mid_layer.add_module(f'Mid_{cls.__name__}_{number}', layer)\n",
        "        else:\n",
        "            self.mid_layer = None\n",
        "\n",
        "        MiddleLayer._number += 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        if self.mid_layer:\n",
        "            return self.mid_layer(x)\n",
        "        else:\n",
        "            return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wWTg55Bbu3w"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, batch_norm = False,\n",
        "                 conv_layers = None,\n",
        "                 middle_layers = None,\n",
        "                 fc_layers = None):\n",
        "        '''\n",
        "        Generate architecture of neural net.\n",
        "        batch_norm - bool. If True, add batchnorm to input\n",
        "        conv_layers - list with conv layers\n",
        "        middle_layers - list with middle layers (mb bottleneck/flatten)\n",
        "        fc_layers - list with fc layers\n",
        "        '''\n",
        "\n",
        "        super().__init__()\n",
        "        if batch_norm:\n",
        "            self.batch_norm = nn.BatchNorm2d(3) # Для RGB картинок.\n",
        "        else:\n",
        "            self.batch_norm = None\n",
        "        self.conv_layers = nn.Sequential(*conv_layers)\n",
        "        self.middle_layers = nn.Sequential(*middle_layers)\n",
        "        self.fc_layers = nn.Sequential(*fc_layers)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        if self.batch_norm:\n",
        "            x = self.batch_norm(x)\n",
        "        \n",
        "        if self.conv_layers:\n",
        "            x = self.conv_layers(x)\n",
        "\n",
        "        if self.middle_layers:\n",
        "            x = self.middle_layers(x)\n",
        "\n",
        "        if self.fc_layers:\n",
        "            x = self.fc_layers(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def readable_config(self):\n",
        "        # Saving readable config\n",
        "        x = cnn.children()\n",
        "        ans = ''\n",
        "        for y in x:\n",
        "            ans += str(y) + '\\n\\n'\n",
        "        return ans"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SnSm8eSra48Z",
        "8dyv0_7gbQyx",
        "Q547p3nH9Llz",
        "lhrc_O1H6wGT",
        "62u_hQUT7Dvb",
        "8tPpz2ClN7wH",
        "uD8IxRDrZNqC",
        "GILf8neHv0gE",
        "Q7XNEgDVwbTA"
      ],
      "provenance": [],
      "mount_file_id": "1MYJbelX_W1qxkE1U6m4k7-lf8x-eHZqh",
      "authorship_tag": "ABX9TyOYJdOosq0AV68ON8VdIIst",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}